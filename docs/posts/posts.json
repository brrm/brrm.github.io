[
  {
    "path": "posts/distributed-ml/",
    "title": "Distributed Machine Learning: How Does it Work?",
    "description": "Distribution has become essential to training large models.\nModern machine learning frameworks provide drop-in distribution, but few understand how these work -- and why they might be slowing down training.",
    "author": [
      {
        "name": "Bruce Mauger",
        "url": {}
      }
    ],
    "date": "2024-12-26",
    "categories": [],
    "contents": "\n\nContents\n1. PyTorch mental model\nHow do you train a PyTorch neural network?\nThe Transformer architecture\n\n2. Collective Communication Primitives\n\nMotivation: models & clusters are really big.\n\nWe’ll take a look at how parallelisation paradigms have evolved to cope with more data, more parameters and more GPUs.\nI intended this as a light compilation of various sources; for more detail, I provide references to the relevant resources throughout.\n[Outline of sections]\nThough I intend to stay light on implementation specifics, the abstractions provided by PyTorch are nonetheless useful for illustration.\nWe’ll be using PyTorch to illustrate and discuss implementation details.\nThe first section provides a rough overview of the PyTorch model, particularly for transformers. If you’re already familiar, skip to the next section.\n1. PyTorch mental model\nPytorch is the preeminent framework, though the stuff here also (mostly) applies to other frameworks.\nUseful for understanding how data flows throughout training.\nHow do you train a PyTorch neural network?\n[Further Reading]\nPyTorch’s fundamental data structure is the Tensor, a multi-dimensional matrix (think NumPy’s ndarray) used to store a model’s parameters and encode inputs/outputs.\nIn PyTorch, a neural network is a Module composed from other modules (layers).\nFor example, here’s a simple network with two linear layers and a ReLU activation in-between:\n\nfrom torch import nn\n\nclass NeuralNetwork(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.linear1 = nn.Linear(2, 4)\n    self.activation_fn = nn.ReLU()\n    self.linear2 = nn.Linear(4, 1)\n  \n  def forward(self, x):\n    x = self.linear1(x)\n    x = self.activation_fn(x)\n    x = self.linear2(x)\n    return x\n\nUnsurprisingly, forward defines the network’s forward pass: how inputs are mapped to outputs. Here a 2D input is mapped to a 1D output, with a 4D “hidden” layer. Taking the first Linear submodule as an example, it holds weight and bias tensors of shapes [4,2] and [4] respectively. Adding the second linear layer’s parameters (the activation function doesn’t have any), we can see the network has a total of 17 trainable parameters.\n\nThis is all well and good, but we can’t actually train the network yet!\nFor that we need a basic training loop:\n\nmodel = NeuralNetwork() \noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nX = torch.ones(10, 2) # input batch tensor\ny = torch.zeros(10, 1) # expected output batch tensor\n\nepochs = 10\nfor t in range(epochs): \n  # Compute prediction and loss\n  pred = model(x)\n  loss = torch.nn.functional.cross_entropy(pred, y)\n  \n  # Update parameters\n  loss.backward()\n  optimizer.step()\n  optimizer.zero_grad()\n\nWe train our model for 10 epochs (iterations) on a single batch of 10 (identical) datapoints1.\nIn each epoch:\nWith model(x), we call the forward method defined earlier to obtain predictions for the entire input batch.\nWe compute the (cross entropy) loss for these predictions and store them in the loss tensor.\nWe calculate the derivative of the loss with respect to each parameter with loss.backward(). PyTorch’s autograd does this automatically by building a computational graph in the forward pass, and then applying backpropagation in the backward pass. It accumulates gradients in each tensor’s .grad attribute2.\nThe optimizer defines how parameters are updated from gradients. optimizer.step() performs this adjustment, and optimizer.zero_grad() resets gradients so they don’t accumulate in the next pass.\nFor each parameter in our network, we also need to store its gradient and relevant optimizer state.\nThe popular Adam optimizer tracks momentum and variance, exponential averages of the first and second moments respectively of each parameter’s gradient [1].\nThe result is that each parameter can end up needing as much as 16 bytes of memory (assuming fp16/32 mixed precision) [2].\nFor larger models such as Meta’s Llama 405B that’s a 6.5TB memory requirement, which makes distributing model parameters over several GPUs a necessity.\nThe Transformer architecture\n[Further Reading]\nThe largest models trained today are transformers.\nNaturally, distributed training has evolved around the architecture, making it a valuable mental model.\n2. Collective Communication Primitives\nLet’s start with a very simple model: two GPUs (or “ranks”) with a point-to-point (p2p) connection – this could be a NVLink interconnect if they’re within the same host, or a slower InfiniBand or Ethernet network (perhaps with several hops) if they’re not (more on this later).\nAll collectives operate over a single tensor.\nThe simplest thing we can do is to send from one rank and receive on the other:\n\nP2P send\nNow let’s suppose we want to synchronise four ranks.\nOne way to do this is with an AllToAll collective, a complete graph of p2p sends:\n\\(N=4\\) rank alltoall\nThis isn’t always super efficient: \\(N\\) ranks require \\(\\mathcal O(N^2)\\) p2p communications, some of which may share underlying network links and bottleneck one another.\nMoreover, we often only need an aggregate of the distributed tensors – for example we might want to average some parameters we’ve replicated across ranks.\nSo how might accomplish this with less bandwidth?\nLet’s start by reducing (e.g. taking the sum, min, max, etc.) the received tensor with the local tensor, before passing the result onto the next rank.\nWhat we obtain is a ring-based Reduce collective:\n\\(N=4\\) rank reduce\nAfter completing one loop around the ring, we’ve reduced the entire tensor – but the result is only held in the last rank.\nWe need to complete another loop so that each rank holds a replica of the complete tensor.\nThis is the Broadcast collective:\n\\(N=\\) rank broadcast\nSequencing the two collectives forms an AllReduce.\nNotice that only one rank at a time is busy, with the rest idle.\nWe can use pipelining to do even better: we split the tensor into \\(N\\) chunks, with the \\(i^\\text{th}\\) rank at the start (or root) of the ring corresponding to the \\(i^\\text{th}\\) chunk.\n\\(N=4\\) rank allreduce\nThe pipelined analogs of Reduce and Broadcast (the first and second loops in the above animation) are ReduceScatter and AllGather respectively.\nYou can see this is exactly the same as an AllToAll, followed by local reductions at each rank.\nBy using a ring, AllReduce improves communication overhead by an order of magnitude to \\(\\mathcal O(n)\\).\nYou may have noticed the ring takes a lot longer, however. In practice we split the tensor so that each rank is always sending, amortising latency penalty (covered later). Though bandwidth optimal, tree-based AllReduce is latency optimal.\n\n\n\n\n\n\n\n1. D. P. Kingma & J. Ba, Adam: A method for stochastic optimization. (2017).\n\n\n2. S. Rajbhandari, J. Rasley, O. Ruwase, & Y. He, ZeRO: Memory optimizations toward training trillion parameter models. (2020).\n\n\nIn practice an epoch will loop over an entire training set consisting of several batches (each with their own parameter updates), potentially followed by evaluation on separate validation batches.↩︎\nSome tensors in a module don’t have gradients, for example fixed transformers with static parameters.↩︎\n",
    "preview": {},
    "last_modified": "2025-01-03T18:50:39+01:00",
    "input_file": "distributed-ml.knit.md"
  },
  {
    "path": "posts/boosting-for-causal-effect-estimation/",
    "title": "Boosting for Causal Effect Estimation",
    "description": "Can you boost when you don't have labels?",
    "author": [
      {
        "name": "Bruce Mauger",
        "url": {}
      }
    ],
    "date": "2024-01-27",
    "categories": [],
    "contents": "\n\nContents\nBackground\nUplift Trees\nBoosting\nBackground\nAdaBoost\n\nBoosting Uplift Trees\nConclusion\nProof of (\\(\\ref{eq:cateuplift}\\))\nTree Splitting Criterion (\\(\\ref{eq:utobjective}\\))\n\n\nThis post provides a very brief overview of my own, original adaptation\nof the AdaBoost algorithm to the problem of (heterogeneous) causal\neffect estimation as part of my undergraduate thesis on causal machine learning.\nIt also includes a light introduction to boosting and\nuplift modelling, though a basic knowledge of probabilities and decision\ntrees is assumed.\nBackground\nUplift modelling is a branch of machine learning that aims to predict\nthe causal effect of an action (“treatment”) on a given individual\n[1]. To illustrate, consider a marketing campaign where\nindividuals either do or don’t receive a particular ad (the treatment),\nand the outcome is whether they made a purchase. The goal is to select\nindividuals most likely to respond positively to the campaign. Causal\neffect estimation has also seen use in many fields outside of marketing,\nincluding medicine, A/B testing and econometrics.\nWe consider a formalism of the problem in terms of the potential\noutcomes framework [2]. Given a dataset of \\(N\\) independent and\nidentically distributed \\((X_i, W_i, Y_i)\\) where \\(X_i \\in \\mathcal{X}\\) are\nper-individual features, while \\(W_i \\in \\{0, 1\\}\\) and \\(Y_i \\in \\{0, 1\\}\\)\ndenote the individual’s treatment assignment and outcome respectively.\nLet \\(\\{Y_i(0), Y_i(1)\\}\\) be the potential outcomes we would have\nobserved had individual \\(i\\) be assigned treatment \\(W_i=0\\) or \\(1\\)\nrespectively. The causal effect of treatment on individual \\(i\\) is the\nIndividual Treatment Effect (ITE), defined as:\n\\[\\tau_i := Y_i(1) - Y_i(0)\\]\nUnfortunately, ITE is not identifiable as we cannot simultaneously\nobserve both potential outcomes: we only observe \\(Y_i=Y_i(W_i)\\) (this is\noften referred to as the Fundamental Problem of Causal Inference).\nInstead, we seek to estimate the conditional average treatment effect\n(CATE):\n\\[\\tau(\\mathbf x) := \\mathbb E[Y(1) - Y(0) \\mid X=\\mathbf x]\\]\n\nAssumption 1 (Unconfoundedness). Potential outcomes are independent of treatment assignment:\n\\(\\{Y_i(0), Y_i(1)\\} \\perp W_i \\mid X_i\\)\n\nIn order to identify the CATE, we assume unconfoundedness (Assumption\n1) which yields the result (proof in the Appendix):\n\\[\\begin{align}\n\\tau(\\mathbf x) &= \\mathbb E[Y(1) \\mid X=\\mathbf x] - \\mathbb E[Y(0) \\mid X=\\mathbf x]  \\nonumber \\\\\n    &= \\mathbb E[Y \\mid W=1, X=\\mathbf x] - \\mathbb E[Y \\mid W=0, X=\\mathbf x] \\label{eq:cateuplift}\n\\end{align}\\]\nwhich is often referred to as the uplift. Unconfoundedness means that\n\\(W_i\\) only affects which of the potential outcomes\n\\(\\{Y_i(0), Y_i(1)\\}\\) is observed, without affecting how they are each\ngenerated.\nThe uplift literature will often make another, stronger assumption that\nthe data originates from a randomised control trial (RCT):\n\nAssumption 2 (Randomised Treatment Assignment).\n\\(e(X_i) = 0.5 = (1-e(X_i))\\)\n\nwhere we write \\(e(X_i) = \\Pr(W_i = 1 \\mid X_i)\\) for the treatment\npropensity. That is, each individual is equally likely to be assigned to the treatment or control group.\nUplift Trees\nThe straightforward way to model uplift is to estimate\n\\(\\mathbb E[Y| W=1, X=\\mathbf x]\\) and \\(\\mathbb E[Y| W=1, X=\\mathbf x]\\)\nseparately. However, since it is not the absolute values of the\nresponses, but the differences in potential outcomes that matter when\nestimating treatment effect, modelling \\(\\tau(\\mathbf x)\\) directly can\nproduce better results.\nTo that end, Rzepakowski and Jaroszewicz propose a modified decision\ntree for direct uplift estimation [3]. Their procedure is\nsimilar to fitting a normal decision tree, in that the model is built by\ngreedily partitioning \\(\\mathcal X\\) into regions \\(R_1, \\ldots, R_J\\) (each\nof which corresponds to a leaf node in the tree) according to a\nsplitting criterion.\nSince the more ubiquitous splitting criteria based on maximising class\nhomogeneity don’t apply (recall: we can’t observe \\(\\tau_i\\)), the authors\npropose several criteria based on maximising the divergence between the\ntreatment and control distributions \\((Y|W=1), (Y|W=0)\\). They argue that,\nof these, Euclidean distance is superior because of its symmetry and\nstability. For a binary tree with binary \\(W_i\\), this reduces to\nmaximising (further details in the Appendix):\n\\[\n\\begin{equation}\n    \\mathcal C^\\mathrm{Euclidean} :=\n\\Pr(\\mathbf x \\in R_\\mathrm{Left}) \\hat\\tau(R_\\mathrm{Left})^2 +\n\\Pr(\\mathbf x \\in R_\\mathrm{Right}) \\hat\\tau(R_\\mathrm{Right})^2 \\label{eq:utobjective} \\end{equation}\n\\]\nwhen splitting into left and right child nodes with regions\n\\(R_\\mathrm{Left}, R_\\mathrm{Right}\\). We write \\(\\hat\\tau(R_j)\\) for the\nempirical average treatment effect in region \\(R_j\\) (over our training\nset):\n\\[\n\\begin{equation}\n\\hat \\tau(R_j) := \\mathbb E[Y \\mid W=1, \\mathbf x \\in R_j] - \\mathbb E[Y \\mid W=0, \\mathbf x \\in R_j]\n\\end{equation}\n\\]\nThe prediction at each leaf node of the tree is then the empirical\naverage treatment effect in the associated region.\nExample fit of Rzepakowski and Jaroszewicz’ decision treeLastly, Rzepakowski and Jaroszewicz also propose a regularisation\ntechnique penalising splits that produce imbalanced treatment and\ncontrol groups.\nBoosting\nSome of the best performing models on classical machine learning\nproblems are those based around boosting an ensemble of weak learners\n(usually trees). Thus, a natural progression from Rzepakowski and\nJaroszewicz’ decision tree is to ask whether it can be boosted.\nBackground\nBoosting takes a basis function \\(h : \\mathcal X \\to \\mathbb R\\) and\nproduces an additive model\n\\(H_T(\\mathbf x) = \\sum_{t=1}^T \\alpha_t h_t (\\mathbf x)\\). Typically, the\nensemble is fit by minimising a (convex and differentiable) loss\nfunction \\(L\\) over the training data:\n\\[\n\\begin{equation}\n\\label{eq:boosting}\n    \\min_{\\{\\alpha_t, h_t \\}_1^T} \\sum_{i=1}^N L(y_i, \\sum_{t=1}^T \\alpha_t h_t(\\mathbf x_i))\n\\end{equation}\n\\]\nOften, this is not a feasible computation. Forward Stagewise Additive\nModelling (FSAM) approximates the solution to\n(\\(\\ref{eq:boosting}\\)) by greedily fitting each basis function. We\nstart by initialising \\(H_0(\\mathbf x) := 0\\). Then, at each iteration \\(t\\)\nwe solve for the optimal basis function \\(h_t\\) and corresponding weight\n\\(\\alpha_t\\), given the functions we have already fit:\n\\[\n\\begin{equation}\n\\label{eq:fsam}\n    h_{t+1}, \\alpha_{t+1} = \\mathrm{argmin}_{h \\in \\mathcal H, \\alpha \\in \\mathbb R^+} \\sum_{i=1}^N L(y_i, H_t(\\mathbf x_i)+\\alpha h(\\mathbf x_i))\n\\end{equation}\n\\]\nand update our ensemble: \\(H_{t+1} := H_t + \\alpha_{t+1} h_{t+1}\\).\nOne way to solve for (\\(\\ref{eq:fsam}\\)) is gradient descent in functional space. We\nwrite \\(\\ell(H_t) = \\sum_{i=1}^N L(y_i, H_t(x_i))\\) for our total loss. Fixing a\nsmall step-size \\(\\alpha\\), we can use a Taylor approximation on\n\\(\\ell(H_t + \\alpha h)\\) to find an almost optimal \\(h\\):\n\\[\n\\begin{align}\n    \\mathrm{argmin}_{h \\in \\mathcal H} \\ell (H_t + \\alpha h)\n    &\\approx \\mathrm{argmin}_{h \\in \\mathcal H} \\;\\ell (H_t) + \\alpha \\langle \\nabla \\ell (H_t), h \\rangle \\nonumber \\\\\n    &= \\mathrm{argmin}_{h \\in \\mathcal H} \\; \\langle \\nabla \\ell (H_t), h \\rangle \\nonumber\\\\\n    &= \\mathrm{argmin}_{h \\in \\mathcal H} \\sum_{i=1}^N \\frac{\\partial \\ell}{\\partial [H_t(\\mathbf x_i)]} h(\\mathbf x_i) \\label{eq:vecapprox}\\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N \\underbrace{-\\frac{\\partial \\ell}{\\partial [H_t(\\mathbf x_i)]}}_{t_i} h(\\mathbf x_i) \\label{eq:graddesc}\n\\end{align}\n\\]\nIn (\\(\\ref{eq:vecapprox}\\)), we use the fact that a function \\(f(\\cdot)\\)\nin our functional space is completely specified by its values on our\ntraining set \\(f(\\mathbf x_1), \\ldots, f(\\mathbf x_n)\\), and hence can be\nconsidered a vector \\(\\vec f \\in \\mathbb R^N\\).\nThe final result (\\(\\ref{eq:graddesc}\\)) has the intuitive interpretation of finding\nthe basis function \\(h\\) closest to the negative gradient of the loss\n\\(\\vec t\\). Note that \\(h\\) need not be perfectly optimal to make progress.\nAs long as \\(\\sum_{i=1}^N t_i h(\\mathbf x_i) > 0\\) (that is, the basis\nfunction & negative gradient lie on the same side of the hyperplane),\nour loss \\(\\ell\\) will decrease. This is the weak learner condition: our\nbasis function must be better than a random function \\(h_R\\), for which we\nwould expect \\(\\sum_{i=1}^N t_i h_R(\\textbf x_i) = 0\\).\nAdaBoost\nOne of the most popular, and earliest, boosting algorithms is the\nAdaBoost algorithm [4]. It was\nlater shown to be a special case of FSAM, with \\(y_i \\in \\{-1, 1\\}\\),\n\\(h(\\mathbf x_i) \\in [-1,1]\\) 1 and optimising for exponential loss [6]:\n\\[\n\\begin{equation}\nL(y, f(x)) = \\exp({-y f(x)})\n\\end{equation}\n\\]\nFirst, we show that \\(\\ell(H_t)\\) at each iteration is equivalent to\nminimising loss for the basis function under a re-weighted distribution:\n\\[\n\\begin{align}\n    h_{t+1}\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N t_i h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N -\\left[\\frac{\\partial}{\\partial H_t(\\mathbf x_i)} \\sum_{i=1}^N \\exp(-y_iH_t(\\mathbf x_i))\\right] h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i)) y_i h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H}\\; \\frac{1}{\\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i))} \\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i)) y_i h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\mathbb E_{i \\sim D_t} [y_i h(\\mathbf x_i)] \\label{eq:adaobjective} \\\\\n\\end{align}\n\\]\nwhere\n\\(D_t(i) = \\frac{\\exp(-y_i H_t(\\mathbf x_i))}{\\sum_{j=1}^N \\exp(-y_j H_t(\\mathbf x_i))} = \\frac{1}{Z_t}\\exp(-y_i H_t(\\mathbf x_i))\\)\nis the weight associated with \\(i^\\mathrm{th}\\) training sample. Note that\nthe normalisation factor \\(Z_t\\) is identical to the total loss\n\\(\\ell(H_t)\\). Each weight \\(D_t(i)\\) can be interpreted as the relative\ncontribution of the \\(i^\\text{th}\\) training sample to the total loss.\nMoreover, in the discrete case where \\(h(\\mathbf x_i) \\in \\{-1,1\\}\\), the\nlearning objective\n(\\(\\ref{eq:adaobjective}\\)) is equivalent to maximising accuracy under\nthe re-weighted distribution.\nMost boosting algorithms do not yield a tractable solution for the\noptimal step-size \\(\\alpha_{t+1}\\) (hence \\(\\alpha\\) is often left as a\nfixed hyperparameter). However, AdaBoost is exceptional in that we can\nfind a (near) optimal step-size. A consequence is that AdaBoost\nconverges fast and overfits slowly [6].\nIn order to find the optimal \\(\\alpha_{t+1}\\), consider the FSAM\nminimisation procedure (\\(\\ref{eq:fsam}\\)):\n\\[\n\\begin{align}\n    \\alpha_{t+1}\n    &= \\mathrm{argmin}_{\\alpha \\in \\mathbb R^+} \\sum_{i=1}^N \\exp(-y_i (H_t(\\mathbf x_i) + \\alpha h(\\mathbf x_i)))  \\nonumber\\\\\n    &= \\mathrm{argmin}_{\\alpha \\in \\mathbb R^+} \\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i)) \\exp(-\\alpha y_i h(\\mathbf x_i)) \\nonumber\\\\\n    &= \\mathrm{argmin}_{\\alpha \\in \\mathbb R^+} \\mathbb E_{i \\sim D_t} [\\exp(-\\alpha y_i h(\\mathbf x_i))] \\label{eq:alphaobjective} \\\\\n\\end{align}\n\\]\nSchapire and Singer propose an upper bound on the objective\n(\\(\\ref{eq:alphaobjective}\\)):\n\\[\n\\begin{equation}\n\\mathbb E_{i \\sim D_t}[\\exp(-\\alpha y_i h(\\mathbf x_i))] \\leq \\mathbb E_{i \\sim D_t}\\left[\\frac{1+y_i h(\\mathbf x_i)}{2} e^{-\\alpha} + \\frac{1-y_i h(\\mathbf x_i)}{2} e^{\\alpha}\\right]\n\\end{equation}\n\\]\nthis upper bound is valid since \\(y_i h(\\mathbf x_i) \\in [-1, 1]\\) 2. The\nstep-size minimising the upper bound can be found analytically, giving:\n\\[\n\\begin{equation}\\alpha_{t+1} = \\frac{1}{2} \\ln (\\frac{1+r_{t+1}}{1-{r_{t+1}}})\\end{equation}\n\\]\nwhere \\(r_{t+1} = \\mathbb E_{i \\sim D_t} [y_i h(\\mathbf x_i)]\\).\nBoosting Uplift Trees\nWe now return to our original question: is it possible to apply a\nboosting algorithm (in particular, the AdaBoost algorithm) to\nRzepakowski and Jaroszewicz’ uplift decision tree? Once again, the\nFundamental Problem of Causal Inference adds a difficulty: there is no\nobvious labelling to use as we cannot observe the ground truth \\(\\tau_i\\).\nLet us consider the following simple class transformation:\n\\[\n\\begin{equation}\n\\hat Y_i :=\n    \\begin{cases}\n        +1 & \\mathrm{if}\\; W_i = Y_i \\\\\n        -1 & \\mathrm{otherwise}\\; (W_i \\neq Y_i)\n    \\end{cases}\n\\end{equation}\n\\]\nWe naïvely assume there is a positive causal effect\nfor positive outcomes in the treatment group or negative outcomes in the\ncontrol group, and a negative causal effect otherwise.\nSince \\(\\hat Y_i \\in \\{-1, 1\\}\\), it is a suitable label for AdaBoost. To\nfind the function that would be estimated by the model, we first examine\nthe exponential loss population minimiser:\n\\[\n\\begin{equation}\nf^*(x) = \\mathrm{argmin}_{f(x)}\\mathbb E_{Y \\mid x}\\exp{(-Y f(x))} = \\frac{1}{2} \\log \\frac{\\Pr(Y=1 \\mid x)}{\\Pr(Y=-1 \\mid x)}\n\\end{equation}\n\\]\nwhich can easily be found analytically.\nThe relevant probability for \\(\\hat Y\\) is:\n\\[\n\\begin{align}\n    \\Pr(\\hat Y = 1 | X = \\mathbf x)\n    ={}& e(\\mathbf x)\\Pr(Y = 1 | W=1, X = \\mathbf x) \\nonumber \\\\\n    &+ (1-e(\\mathbf x))\\Pr(Y = 0 | W=0, X=\\mathbf x) \\nonumber \\\\\n    ={}& e(\\mathbf x)\\mathbb E[Y \\mid W=1, X=\\mathbf x] \\nonumber \\\\\n    &+ (1-e(\\mathbf x))(1-\\mathbb E[Y \\mid W=0, X=\\mathbf x])\n\\end{align}\n\\]\nIf we assume random treatment assignment (Assumption\n2), this gives:\n\\[\n\\begin{align}\n    \\Pr (\\hat Y = 1 | X = \\mathbf x)\n    &= \\frac{1}{2}\\mathbb E[Y \\mid W=1, X=\\mathbf x] + \\frac{1}{2}(1-\\mathbb E[Y \\mid W=0, X=\\mathbf x]) \\nonumber \\\\\n    &= \\frac{1}{2} + \\frac{1}{2} \\tau (\\mathbf x)\n\\end{align}\n\\]\nIn fact, under random treatment assignment \\(\\hat Y\\) is\nan unbiased estimator of \\(\\tau(\\mathbf x)\\):\n\\[\n\\begin{align}\n    \\mathbb E[\\hat Y = 1 | X= \\mathbf x]\n    &= (\\frac{1}{2} + \\frac{1}{2}\\tau(\\mathbf x)) - (\\frac{1}{2} - \\frac{1}{2}\\tau(\\mathbf x)) \\nonumber \\\\\n    &= \\tau(\\mathbf x)\n\\end{align}\n\\]\nHence, AdaBoost with labels \\(\\hat Y_i\\) yields the additive model:\n\\[\n\\begin{align}\n    H_T(\\mathbf x)\n    &\\approx \\mathrm{argmin}_{\\{\\alpha_t, h_t\\}_1^T} \\mathbb E_{\\hat Y \\mid \\mathbf x}\\exp\\left ({-\\hat Y \\left [\\sum_{t=1}^T \\alpha_t h_t(\\mathbf x)\\right ]} \\right ) \\nonumber \\\\\n    &=\\frac{1}{2} \\log \\frac{\\Pr(\\hat Y = 1 |X=\\mathbf x)}{\\Pr(\\hat Y = -1 \\mid X=\\mathbf x)} \\nonumber \\\\\n    &=\\frac{1}{2} \\log \\frac{1+ \\tau(\\mathbf x)}{1 - \\tau(\\mathbf x)}\n\\end{align}\n\\]\nWhich with a simple transformation gives us the desired\nestimator:\n\\[\n\\begin{equation}\n\\hat H_T(\\mathbf x) := 2 \\left[\\frac{1}{1+e^{-2 H_T(\\mathbf x)}}\\right] - 1 \\approx \\tau(\\mathbf x)\n\\end{equation}\n\\]\nHaving demonstrated that AdaBoost with labels \\(\\hat Y_i\\) can directly\nmodel uplift, we now show that Rzepakowski and Jaroszewicz’ uplift\ndecision tree is a suitable weak learner.\nThe first requirement, that \\(h(\\mathbf x_i) \\in [-1, 1]\\), is trivially\nsatisfied: for any leaf node \\(R_j\\) we have \\(\\hat \\tau(R_j) \\in [-1, 1]\\).\nNext, we show that, after re-weighting, the basis function objective\n(\\(\\ref{eq:adaobjective}\\)) with labels \\(\\hat Y\\) is approximately the\nsame as the decision tree fitting objective\n(\\(\\ref{eq:utobjective}\\)):\n\\[\n\\begin{align}\n    \\mathbb E_{i \\sim D_t}[\\hat Y_i h(\\mathbf x_i)]\n    &= \\sum_{j=1}^J \\mathbb E_{i \\sim D_t}[\\hat Y_i h(\\mathbf x_i) | \\mathbf x_i \\in R_j] \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j) \\label{eq:sumnodes}\\\\\n    &= \\sum_{j=1}^J \\hat \\tau(R_j) \\mathbb E_{i \\sim D_t}[\\hat Y_i | \\mathbf x_i \\in R_j] \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j) \\nonumber \\\\\n    &= \\sum_{j=1}^J \\hat \\tau(R_j) \\mathbb E_{i \\sim D_t} [\\tau(\\mathbf x_i) | \\mathbf x_i \\in R_j] \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j) \\label{ass:objrct} \\\\\n    &= \\sum_{j=1}^J \\hat \\tau(R_j)^2 \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j)\n\\end{align}\n\\]\nwhere in (\\(\\ref{eq:sumnodes}\\)) we apply the law of total probability to sum\nover the tree’s leaf nodes.\nThe reasons why the tree only approximates the objective\n(\\(\\ref{eq:adaobjective}\\)) are twofold. First, splits are chosen\ngreedily. Second, (\\(\\ref{ass:objrct}\\)) relies on (Assumption 2). Even if it holds\non the dataset (the root node), it may not hold within child nodes.\nHowever, the regularisation proposed by Rzepakowski and Jaroszewicz can\nalleviate this. Moreover, as previously discussed we only require that\nour basis function be better than random, thus in practice the tree\nremains suitable.\n\nAlgorithm Uplift AdaBoostInput: training set \\(\\{(\\mathbf x_i, w_i, y_i)\\}\\), number of iterations \\(T\\)\nSet boosting labels \\(\\hat y_i = w_i(2y_i - 1) + (1-w_i)(1 - 2y_i)\\)\nInitialise weights \\(D_1(i) = \\frac{1}{N}, i = 1, \\ldots, N\\)\nFor \\(t=1\\) to \\(T\\):\nFit uplift tree \\(h_t\\) with splitting criterion \\(\\mathcal C^\\text{Euclidean}\\) to the training set using weights \\(D_t(i)\\)\nCompute \\(r_t = \\sum_{i=1}^N D_t(i) \\hat y_i h(\\mathbf x_i)\\)\nCompute step-size \\(\\alpha_t = \\frac{1}{2} \\ln(\\frac{1+r_t}{1 - r_t})\\)\n\nUpdate weights \\(D_{t+1}(i) = \\frac{1}{Z_t} D_t(i) \\exp(-\\alpha_t \\hat y_i h_t(\\mathbf x_i))\\)\nOutput ensemble \\(\\hat H_T(\\mathbf x) = 2\\left( {1+\\exp({-2[\\sum_{t=1}^T \\alpha_t h_t(\\mathbf x)]})} \\right)^{-1} - 1\\)\n\nThe final algorithm is a modification of Real AdaBoost with proxy labels\n\\(\\hat y\\), and using Rzepakowski and Jaroszewicz’ uplift tree rather than\na traditional decision tree.\nConveniently, the weight initialisation (step 2) allows us to drop (Assumption 2). If we have a consistent\nestimator of propensity \\(\\hat e(\\mathbf x)\\), we can instead choose\nweights \\(D_1(i) = \\frac{1}{Z}(1/\\hat e(\\mathbf x_i))\\) and \\(D_1(i) = \\frac{1}{Z}(1/(1-\\hat e(\\mathbf x_i)))\\) for treatment and control samples respectively. Under this new distribution (Assumption\n2) holds.\nConclusion\nAfter a brief introduction to uplift modelling, we have shown that it is\ntheoretically possible to boost Rzepakowski and Jaroszewicz’ uplift tree\nwithout observing the ground truths \\(\\tau_i\\). This is achieved using a\nsurprisingly simple and naïve class transformation \\(\\hat Y_i\\). In the\nnext part of this series, we will examine the modified boosting\nalgorithm’s performance, and discuss some of the challenges that come\nwith producing useful evaluation metrics without access to the ground\ntruths.\nProof of (\\(\\ref{eq:cateuplift}\\))\nBy definition:\n\\[\n\\begin{aligned}\n    \\tau(\\mathbf x) ={}& \\mathbb E[Y(1) - Y(0) | X = \\mathbf x] \\\\\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x]e(\\mathbf x) + \\mathbb E[Y(1) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & - \\mathbb E[Y(0) | W=1, X=\\mathbf x]e(\\mathbf x) - \\mathbb E[Y(0) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x]e(\\mathbf x) + \\mathbb E[Y(1) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & - \\mathbb E[Y(0) | W=1, X=\\mathbf x]e(\\mathbf x) - \\mathbb E[Y(0) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & + \\mathbb E[Y(1) | W=1, X=\\mathbf x](1-e(\\mathbf x)) - \\mathbb E[Y(1) | W=1, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & + \\mathbb E[Y(0) | W=0, X=\\mathbf x]e(\\mathbf x) - \\mathbb E[Y(0) | W=0, X=\\mathbf x]e(\\mathbf x) \\\\\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x] - \\mathbb E[Y(0) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & + e(\\mathbf x)\\left( \\mathbb E[Y(0) | W = 0, X=\\mathbf x] - \\mathbb E[Y(0) | W=1, X=\\mathbf x]\\right) \\\\\n    & + (1-e(\\mathbf x)) \\left(\\mathbb E[Y(1) | W=0, X=\\mathbf x] - \\mathbb E[Y(1) | W=1, X=\\mathbf x]\\right)\n\\end{aligned}\n\\]\nUnder (Assumption 1), the observed outcome\n\\(\\mathbb E[Y(w) | W=w, X=\\mathbf x]\\) is the same as the unobserved\n\\(\\mathbb E[Y(1-w) | W=1-w, X=\\mathbf x]\\) which gives us:\n\\[\n\\begin{aligned}\n    \\tau(\\mathbf x)\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x] - \\mathbb E[Y(0) | W=0, X=\\mathbf x] \\\\\n    ={}& \\mathbb E[Y | W=1, X=\\mathbf x] - \\mathbb E[Y | W=0, X=\\mathbf x]\n\\end{aligned}\n\\]\nTree Splitting Criterion (\\(\\ref{eq:utobjective}\\))\nIn their paper, Rzepakowski and Jaroszewicz argue that maximising\nEuclidean distance between treatment and control distributions is the\nbest splitting criterion. For two discrete random variables \\(P,Q\\)\nwith probabilities \\(p_i = \\Pr(P=i), q_i = \\Pr(Q=i)\\) respectively,\ntheir Euclidean distance is defined as:\n\\[D^\\mathrm{Euclidean}(P, Q) = \\sum_i (p_i - q_i)^2\\]\nRzepakowski and Jaroszewicz consider datasets with multiple treatment\ngroups, and trees with \\(n\\)-way splits. If we restrict to binary\ntreatment with binary splits, we are left maximising:\n\\[\n\\begin{aligned}\n    \\mathcal C^\\mathrm{Euclidean}  ={}& D^\\mathrm{Euclidean}((Y|W=1, \\mathbf x \\in R_\\mathrm{Left}), (Y|W=0, \\mathbf x \\in R_\\mathrm{Left})) \\Pr(\\mathbf x \\in R_\\mathrm{Left}) \\\\\n    &+ D^\\mathrm{Euclidean}((Y|W=1, \\mathbf x \\in R_\\mathrm{Left}), (Y|W=0, \\mathbf x \\in R_\\mathrm{Left})) \\Pr(\\mathbf x \\in R_\\mathrm{Left})\n\\\\\n\\end{aligned}\n\\]\nWe can show that:\n\\[\nD^\\mathrm{Euclidean}((Y|W=1, \\mathbf x \\in R_j), (Y|W=0, \\mathbf x \\in R_j)) \\\\\n\\begin{aligned}\n    \\hspace{10em}={}& [\\Pr(Y=1|W=1, \\mathbf x \\in R_j) - \\Pr(Y=1|W=0, \\mathbf x \\in R_j)]^2 \\\\\n    & + [\\Pr(Y=0|W=1, \\mathbf x \\in R_j) - \\Pr(Y=0|W=0, \\mathbf x \\in R_j)]^2 \\\\\n    ={}& [\\mathbb E[Y | W=1, \\mathbf x \\in R_j] - \\mathbb E[Y | W=0, \\mathbf x \\in R_j]]^2 \\\\\n    & + [(1-\\mathbb E[Y | W=1, \\mathbf x \\in R_j]) - (1-\\mathbb E[Y | W=1, \\mathbf x \\in R_j])]^2 \\\\\n    ={}& 2 \\hat \\tau (R_j)^2\n\\end{aligned}\n\\]\nHence:\n\\[\n\\mathcal C^\\mathrm{Euclidean} = 2 [\\hat \\tau(R_\\mathrm{Left})^2 \\Pr(\\mathbf x \\in R_\\mathrm{Left}) + \\hat\\tau(R_\\mathrm{Right})^2 \\Pr(\\mathbf x \\in R_\\mathrm{Right})]\n\\]\n(in (\\(\\ref{eq:utobjective}\\)) we drop the redundant constant).\n\n\n\n1. P. Gutierrez & J.-Y. Gérardy, Causal inference and uplift modelling: A review of the literature. In C. Hardgrove, L. Dorard, K. Thompson, & F. Douetteau,eds., Proceedings of the 3rd international conference on predictive applications and APIs (PMLR, 2017), pp. 1–13.\n\n\n2. D. Rubin, Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66 (1974). https://doi.org/10.1037/h0037350.\n\n\n3. P. Rzepakowski & S. Jaroszewicz, Decision trees for uplift modeling with single and multiple treatments. Knowledge and Information Systems, 32 (2012) 303–327.\n\n\n4. Y. Freund & R. E. Schapire, A desicion-theoretic generalization of on-line learning and an application to boosting. In P. Vitányi,ed., Computational learning theory (Berlin, Heidelberg: Springer Berlin Heidelberg, 1995), pp. 23–37.\n\n\n5. R. E. Schapire & Y. Singer, Improved boosting algorithms using confidence-rated predictions. Proceedings of the eleventh annual conference on computational learning theory (New York, NY, USA: Association for Computing Machinery, 1998), pp. 80–91. https://doi.org/10.1145/279943.279960.\n\n\n6. T. Hastie, R. Tibshirani, & J. H. Friedman, The elements of statistical learning: Data mining, inference, and prediction (Springer, 2009).\n\n\nHere we refer to the Real AdaBoost algorithm [5] that relaxes the original \\(h(\\mathbf x_i) \\in \\{-1, 1\\}\\) assumption of Discrete AdaBoost [4].↩︎\nIn fact, it is an exact bound for Discrete Adaboost since\n\\(y_i h(\\mathbf x_i) \\in \\{-1, 1\\}\\).↩︎\n",
    "preview": {},
    "last_modified": "2024-12-26T18:32:53+01:00",
    "input_file": {}
  }
]
