[
  {
    "path": "posts/distributed-ml/",
    "title": "Distributed Machine Learning: How Does it Work?",
    "description": "State of the art AI requires orchestrating large clusters to perform a single synchronised calculation. \nHow does this orchestration work? And how can it be done without incurring expensive communication overheads?",
    "author": [
      {
        "name": "Bruce Mauger",
        "url": {}
      }
    ],
    "date": "2025-01-01",
    "categories": [],
    "contents": "\n\nContents\n1. PyTorch mental model\nHow do you train a PyTorch neural network?\nThe Transformer architecture\n\n2.Communication Primitives\n3. Parallelism Paradigms\nDistributed Data Parallel (DDP)\nFully-Sharded Data Parallel (FSDP)\nPipeline Parallel (PP)\nTensor Parallel (TP)\nContext Parallel (CP)\n\n4. Parallelism in Practice\n\nMany recent advances in AI are owed to training of larger and larger models.\nThese neural networks are too big fit in and take too long to train with a single GPU.\nState of the art AI therefore requires orchestrating vast clusters of GPUs to perform a single synchronised calculation, with the objective of:\nReducing memory impact, so we can fit larger models.\nIncreasing degree of parallelism, so we can use lots of compute in parallel to speed up training.\nThere’s no such thing as a free lunch: by distributing training we incur a communication overhead when GPUs have to talk to each other.\nMeta’s breakthrough Llama 3 model was trained1 on a cluster of 24K GPUs, but with a per-GPU utilisation well below 50% [1].\nWe’ll be taking an in-depth look at the wide variety of parallelism paradigms that have enabled training of gigantic models.\nAs we’ll see, it’s relatively easy to design parallelism techniques that achieve both of the previous objectives; doing so without incurring prohibitive communication overheads, however, is a very difficult engineering and research challenge.\nThe communication aspects of these designs will be focus of this post.\nWe assume only basic prior knowledge of neural networks and gradient descent.\nThe first two sections provide background on the PyTorch framework (as well as the transformer model that underpins modern LLMs), and collective communication primitives respectively.\nThe next section presents a deep-dive into all of the major parallelism techniques, including how PyTorch non-intrusively integrates these into its inherently local execution model.\nWe’ll conclude by looking at how parallelisms are composed together in practice, in particular for Llama 3.\n1. PyTorch mental model\nWe’ll be using PyTorch to illustrate model distribution throughout (though everything remains largely applicable to other frameworks).\nIt’s useful to first get an understanding of how data flows during the PyTorch model training process, in order to spot opportunities for parallelisation.\nHow do you train a PyTorch neural network?\n[Further Reading]\nPyTorch’s fundamental data structure is the Tensor, a multi-dimensional matrix (think NumPy’s ndarray) used to store a model’s parameters and encode inputs/outputs.\nIn PyTorch, a neural network is a Module composed by stitching other modules (layers) and functions together.\nFor example, here’s a simple network with two linear layers and a ReLU activation function in-between:\n\nfrom torch import nn\n\nclass NeuralNetwork(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.linear1 = nn.Linear(2, 4)\n    self.activation_fn = nn.ReLU()\n    self.linear2 = nn.Linear(4, 1)\n  \n  def forward(self, x):\n    x = self.linear1(x)\n    x = self.activation_fn(x)\n    x = self.linear2(x)\n    return x\n\nUnsurprisingly, forward defines the network’s forward pass: how inputs are mapped to outputs. Here a 2D input is mapped to a 1D output, with a 4D “hidden” layer. Taking the first Linear submodule as an example, it holds weight and bias tensors of shapes [4,2] and [4] respectively. Adding the second linear layer’s parameters (the activation function doesn’t have any), we can see the network has a total of 17 trainable parameters.\n\nThis is all well and good, but we can’t actually train the network yet!\nFor that we need a basic training loop:\n\nmodel = NeuralNetwork() \noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nX = torch.ones(10, 2) # input batch tensor\ny = torch.zeros(10, 1) # expected output (target) batch tensor\n\nepochs = 10\nfor t in range(epochs): \n  # Compute prediction and loss\n  pred = model(x)\n  loss = torch.nn.functional.cross_entropy(pred, y)\n  \n  # Update parameters\n  loss.backward()\n  optimizer.step()\n  optimizer.zero_grad()\n\nWe train our model for 10 epochs (iterations) over a single batch of \\(B=10\\) (identical) data samples2.\nIn each epoch:\nWith model(x), we call the forward method defined earlier to obtain predictions for the entire input batch. The outputs of each layer (“activations”) are cached for use in the backward pass.\nWe compute the (cross entropy) loss for these predictions and store them in the loss tensor.\nWe calculate the derivative of the loss of each sample with respect to each parameter (“gradients”) with loss.backward(). PyTorch’s autograd does this automatically by building a computational graph in the forward pass, and then applying backpropagation starting from the outer layer in the backward pass. It accumulates gradients in each tensor’s .grad attribute3.\nThe optimizer defines how parameters are updated from gradients. optimizer.step() performs this adjustment, and optimizer.zero_grad() resets gradients so they don’t accumulate in the next pass.\nPebble graph for a four layer network illustrating how cached activations are built up in the forward pass, and used to calculate gradients in the backward pass (graphic inspiration).This process is known as stochastic gradient descent: we iteratively update parameters using gradients calculated over a random subset (batch) of the entire dataset.\nFor each parameter in our network, we also need to store its gradient and relevant optimizer state.\nThe popular Adam optimizer tracks momentum and variance, exponential averages of the first and second moments respectively of each parameter’s gradient [2].\nThe result is that each parameter can end up needing at least 16 bytes of memory, mostly attributable to high-precision optimizer state (assuming fp16/32 mixed precision4) [3].\nFor larger models such as Meta’s Llama 405B that’s a 6.5TB memory requirement, which makes distributing model parameters over several GPUs a necessity.\nPyTorch offers two execution models: eager mode and graph mode.\nIn eager mode (the default), operators are immediately executed as they are encountered – effectively, we can’t “look ahead”.\nGraph mode synthesises operators into a graph, which is then compiled and executed as a whole.\nAs of PyTorch 2.5, most of the parallelisms offered only exist in eager mode – which, as we’ll see, can often lead to suboptimal sequences of operations.\nThe Transformer architecture\n[Further Reading]\n[WIP]\nAs the largest models trained today are transformers, most of the distributed training literature has evolved around this architecture.\nTransformers are usually made of several equal-size transformer blocks, which are very convenient for splitting/parallelisms.\n2.Communication Primitives\n[Further Reading]\nBefore going into distribution strategies, we need to discuss the primitives we have available for communicating data between GPUs.\nLet’s start with a simple model: two GPUs (or “ranks”) with a point-to-point (p2p) connection – this could be a fast NVLink interconnect if they’re within the same host, or a slower InfiniBand or Ethernet network (perhaps with several hops) if they’re not.\nAll primitives operate over a single tensor at each rank.\nThe simplest thing we can do is to send a tensor from one rank and receive on the other:\n\nP2P send, circles correspond to ranks and squares to tensors.\nNow let’s suppose we want to synchronise tensors distributed over a group (or “collective”) of GPUs.\nOne way to do this is with an AllToAll collective, a complete graph of p2p sends:\n\\(W=4\\) rank AllToAll\nThis isn’t very bandwidth efficient: a world-size of \\(W\\) ranks synchronising \\(D\\)-sized tensors results in \\(D(W-1)\\) per-GPU traffic, some of which may be contending for the same underlying network links.\nMoreover, we often only need an aggregate of the distributed tensors – for example we might want to average some parameters we’ve replicated across the ranks.\nSo how might we accomplish this with less bandwidth?\nIf each rank reduces (applying an associative5 operator e.g. sum, min, max, etc.) the tensor it receives with its own local tensor, before passing the result onto the next rank, we obtain a ring-based Reduce collective:\n\\(W=4\\) rank Reduce\nAfter completing one loop around the ring, we’ve reduced all of the tensors into a single tensor – but this result is only held in the last rank.\nWe need to complete another loop so that each rank holds a replica of the resulting tensor.\nThis is the Broadcast collective:\n\\(W=4\\) rank Broadcast\nNotice that, in the latter two collectives, only one rank/link at a time is busy, with the rest idle.\nWe can use pipelining to get better throughput: we split the tensor into \\(W\\) chunks, with the \\(r^\\text{th}\\) rank at the start (or root) of the ring corresponding to the \\(r^\\text{th}\\) chunk.\nThe pipelined analogs of Reduce and Broadcast are ReduceScatter and AllGather respectively.\nSequencing the two together results in the composite AllReduce collective:\n\\(W=4\\) rank AllReduce\nThe ReduceScatter and AllGather collectives correspond to the first and second loops in the above animation.\nNotice we obtain the same result we would have had with an AllToAll followed by local reductions at each rank.\nHowever, with its use of a ring, AllReduce improves communication overhead by an order of magnitude.\nEach GPU will send a \\(\\frac{D}{W}\\)-size datachunk \\(W-1\\) times for the ReduceScatter and \\(W-1\\) times for the AllGather, for a total per-GPU traffic of \\(2(W-1)\\frac{D}{W}\\). Crucially, this is independent of the number of GPUs in the collective!\nThough Ring AllReduce is bandwidth optimal, its end-to-end latency scales linearly with the number of ranks. A lower latency, tree-based alternative will be discussed in another post.\n\n3. Parallelism Paradigms\nNow that we have an understanding of how data flows during the PyTorch model training process and the primitives we have available for communicating this data between GPUs, let’s look at the various techniques that have emerged for distributing training.\nIn order to have some notion of correctness for these techniques, we’ll define a distributed algorithm to be locally consistent if it is mathematically equivalent to local training.\n\nDistributed Data Parallel (DDP)\n[Further Reading]\nAs its name would imply, DDP splits our dataset across ranks (each with an identical copy of the model), with periodic synchronisation to ensure model replicas are consistent. DDP is useful when our model is still small enough to fit on a single GPU, but we’d like to speed up training by having several GPUs work on a single batch in parallel.\nWe described local training in Section 1: at each iteration we load the next batch, perform a forward pass while caching each layer’s activations, and calculate the loss. Then we run the backward pass to calculate gradients, before our optimizer updates parameters.\nLocal training example on a batch of 6 MNIST data samples (image credit).DDP duplicates the model across \\(W\\) ranks, splitting batches into \\(W\\) different \\(\\beta=\\frac{B}{W}\\)-size chunks for each rank to process:\nData parallel training example with \\(W=2\\) ranks (image credit).Without any communication overhead, this should result in a linear \\(W\\times\\) speedup.\nThe forward and backward passes are independent sample-wise calculations6, and hence our batches can be independently processed without any communication.\n\nWhy don’t we synchronise parameters rather than gradients?\nIf we’re using stochasting gradient descent, there wouldn’t be any difference:\n\\[\n\\begin{align*}\n\\theta' ={}& \\theta + \\eta \\nabla \\theta\\\\\n={}& \\theta+\\eta \\frac{1}{W} \\sum_{r=0}^W \\nabla \\theta_r^\\text{local}\\\\\n={}& \\frac{1}{W}\\sum_{r=0}^W (\\theta+\\eta \\nabla \\theta_r^\\text{local})\n\\end{align*}\n\\]\nHowever, state updates for stateful optimizers like Adam are non-linear functions of the gradient, and thus we would lose local consistency as optimizer states diverge.\nTo achieve local consistency, we need to synchronise our gradients before the optimizer step so that the weight updates at each rank are the same.\nConveniently, the most commonly used loss functions are means over the sample-wise losses in the batch:\n\\[\n\\text{loss}(\\text{batch}) = \\frac{1}{B} \\sum_{j=0}^B \\text{loss}(\\text{fwd}(\\text{input}_j), \\text{target}_j)\n\\]\nBecause the gradient of a sum is the sum of the gradients of each term, we can calculate gradients for the chunks at each rank independently and average them together to obtain the gradient over the entire batch:\n\\[\n\\nabla \\theta = \\frac{1}{W}\\sum_{r=0}^W \\nabla \\theta_r^\\text{local}\n\\]\nThis can be done efficiently using the previously discussed AllReduce collective (along with a single Broadcast from the root rank after model construction, to synchronise initial parameters).\nLastly, it’s worth noting that batchsize limits the maximum degree of DDP parallelism.\nWe need to maintain chunk sizes bigger than a minimum batchsize per GPU (\\(\\beta_\\text{min}\\)), below which compute intensity/utilisation decreases substantially.\nOn the other hand, stochastic gradient descent is maximally efficient when overall batchsize is well-below an empirical value known as the critical batch size7, \\(B \\ll B_\\text{crit}\\) [4].\nThis small batchsize regime is unattainable for larger clusters since \\(B \\geq W\\beta_\\text{min} > B_\\text{crit}\\).\nDDP in PyTorch\nPyTorch’s distribution API is designed for non-intrusive scaling out from local training.\nApplying DDP to a model is as simple as wrapping our local model with the DDP nn.Module class: nn.parallel.DDP(model, process_group=...).\nThe process group (PyTorch’s abstraction for a group of processes that run collectives together) allows us to specify what communication backend to use, and which ranks to distribute over.\nCare should be taken to ensure the batches processed by each rank are different (e.g. with the DistributedSampler dataloader class).\nA naive implementation of DDP would synchronise gradients only after running a full forward and backward pass, and then subsequently calling optimizer.step().\nThis is suboptimal as it divides training into two distinct phases: one where we’re waiting for backpropagation to finish computing while the network is idle, and another where the network is communicating as fast as possible while our expensive GPUs are doing (almost) nothing:\nNaive DDP implementation with non-overlapping computation and communication (image credit).Notice in the above that gradients for later layers are already available while we’re still computing the backward pass of earlier layers.\nFor example, the gradients of Layer3 are ready while we’re backpropagating through Layer2.\nThis allows us to overlap computation with (non-blocking) communication, speeding up the complete iteration:\nFaster DDP implementation with overlapping computation and communication (image credit).Collective communications are more efficient on large tensors. Therefore, in practice, rather than launching a dedicated AllReduce immediately as soon as a layer’s gradient tensor is ready, we use Gradient Bucketing: we wait for a short period and bucket multiple tensors at a time into one AllReduce.\nTo non-intrusively integrate with its eager execution model, PyTorch implements DDP by registering one autograd hook (a callback) with each parameter tensor, which fires after the corresponding gradients are updated (during the loss.backward() call).\nOnce all hooks in a bucket8 have fired, an asynchronous AllReduce is triggered. PyTorch’s DDP paper [5] shows interleaving brings significant performance gains, particularly when using the recommended NCCL communication backend:\nPer-iteration normalised latency breakdown, comparing non-overlapping vs overlapping communication; training on 32 GPUs across 4 machines. Figure from [5].Amortised communication overhead can be further reduced with Gradient Accumulation: rather than synchronising gradients every iteration, we accumulate (via the no_sync context manager) the gradients of \\(n\\) local training iterations before synchronising gradients globally and updating parameters.\n[5] claims this enables near-linear scaling for smaller GPU clusters, with “negligible accuracy penalty”:\nPer-iteration latencies (left), and final training loss (right) for \\(n\\) iterations of gradient accumulation. Figure from [5].Fully-Sharded Data Parallel (FSDP)\n[Further Reading]\nDDP speeds up training by distributing our dataset across multiple ranks, but what happens when our model can’t fit within a single GPU?\nDDP’s newer alternative, FSDP, addresses this by also splitting model parameters.\nFSDP is a PyTorch native implementation of DeepSpeed’s ZeRO [6], with some further optimisations.\nFSDP reduces memory footprint by sharding model parameters: the model is split horizontally so that each rank only holds a subset (“shard”) of the parameters (and associated gradients and optimizer state) in any given layer.\nThe naive approach to guaranteeing local consistency is to compute the partial activations of a layer corresponding to the local shard, and then to communicate these activations with the other ranks before proceeding onto the next layer:\nNaive FSDP forward pass: activations are data-dependent and therefore appear on the critical path.The obvious problem with this approach is that communication appears on the critical path: we can’t compute the forward pass for a given layer until we’ve received the complete activations of the previous layer.\nInstead of communicating activations, FSDP’s approach is to communicate parameters.\nFSDP fully materialises parameters before computations, just as in local training, thus removing any data dependency.\nHowever, we would need to be able to materialise parameters on a single GPU, eliminating our memory savings!\nFSDP’s simple solution is to partition the model into groups of layers called units, only instantiating one unit at a time on-demand.\nSo what does this look like in practice?\nLet’s look at a simple six layer model (illustrated below), which we’ve decided to decompose into three units: [layer0, layer3], [layer1, layer2] and [layer4, layer5].\nConsider what happens to unit1 consisting of [layer1, layer2]:\nJust before the forward pass through layer1, we materialise the parameters in unit1 by gathering shards from peer ranks. We can do this with an AllGather (equivalent to each rank Broadcasting its own shard).\nAfter completing local forward computation, we free peer shards (but keep activations).\nBefore the backward pass through layer2, we AllGather the shards again.\nAfter gradients are calculated, we free peer shards and then ReduceScatter to sum up and shard gradients (equivalent to each rank Reducing the gradients in its shard).\nFinally, after completing full forward & backward passes through all units, we update our shard of the parameters in the optimizer step9.\nFSDP example with three units, fully sharded over two ranks. Figure from [7].In effect, FSDP decomposes DDP’s AllReduce into a ReduceScatter and an AllGather in the backward and forward passes respectively – the only extra communication incurred is when we AllGather parameters again during backpropagation.\nSharding Strategies\nFSDP enables fine-grained trade-offs between memory footprint and communication overhead via the sharding factor \\(F\\): the number of ranks over which parameters are sharded.\nBy setting \\(F=W\\) (i.e. the global world size), FSDP fully shards the model with each rank holding only \\(\\frac{1}{W}\\) of the model (as in the above example, with \\(F=W=2\\)).\nHybrid sharding, sharding factors ranging between \\(1\\) and \\(W\\), combines both sharding and replication.\nWe end up with sharding groups \\(S_1, \\ldots, S_\\frac{W}{F}\\), each consisting of \\(F\\) ranks over which parameters are sharded, and replication groups \\(R_1, \\ldots, R_F\\) (directly corresponding to these shards), each consisting of \\(\\frac{W}{F}\\) ranks (one from each sharding group) over which shards are replicated.\nThe AllGather+AllGather+ReduceScatter collectives, previously over all ranks, are now collectives within each sharding group, followed by an AllReduce within each replication group to synchronise gradient shards (as in DDP). This is effectively the decomposition:\n\\[\n\\nabla \\theta = \\frac{1}{W}\\sum_{r=1}^W \\nabla \\theta_r^\\text{local} = \\frac{1}{W}\\sum_{i=1}^{W/F}\\sum_{r \\in S_i}\\nabla \\theta_r^\\text{local}\n\\]\nFor example, with \\(W=16\\) ranks and \\(F=8\\) hybrid sharding, the \\(r=9\\) rank would AllGather parameters and ReduceScatter its gradient shard with peers in the \\(S_2\\) sharding group, before AllReducing the gradient shard with its peer in the \\(R_2\\) replication group:\nFSDP Hybrid Sharding (\\(F=8\\)) example with \\(W=16\\) ranks.You might’ve spotted that setting \\(F=1\\) results in a single replication group (with no memory savings) – this simplifies to vanilla DDP using AllReduce for gradient synchronisation.\nIt’s worth noting that with any sharding strategy, ranks are expected to have distinct input batch chunks (otherwise we’d simply be duplicating gradient calculations).\nThough we can keep reducing memory overhead with larger sharding groups, our degree of compute parallelism will encounter the same batchsize limitations as DDP.\nUsing our traffic calculations from Section 2, the per-GPU communication of an \\(M\\)-size model is \\(2(\\frac{W}{F}-1)(\\frac{M}{W})\\) for the replication group, and \\(3(F-1)(\\frac{M}{F})\\) for the sharding group.\nBecause communication within the sharding group is more expensive (and intertwined with the critical path), we usually try to minimise the number of hops between the ranks in a sharding group – sometimes we may even use smaller sharding factors to ensure they’re within the same host.\nFSDP in PyTorch\nJust like DDP, the FSDP API is designed as a thin nn.Module wrapper class: sharded_model = FSDP(model, process_group=...)10.\nSharding strategy is set with the sharding_strategy arg: FULL_SHARD, NO_SHARD and HYBRID_SHARD correspond to aforementioned fully sharded, fully replicated and hybrid strategies respectively11.\nBefore going into all the other levers that FSDP exposes to the user, let’s first get a quick understanding of how it’s implemented under the hood.\nThe communication backends (e.g. NCCL) that provide collective implementations usually require AllGather and ReduceScatter to have the same input tensor size at each rank.\nMoreover, for a fixed communication volume issuing fewer, larger collectives reduces communication overheads (as discussed in DDP’s Gradient Bucketing).\nThus, during construction FSDP concatenates all parameters (and gradients) within a unit into a single flattened 1-D FlatParameter tensor, along with the padding necessary to ensure equal-sized shards at each rank in the sharding group12.\nThe FlatParameter tensor has the exact data layout expected by AllGather and ReduceScatter, allowing us to call the collectives directly without copying any tensors.\nFlatParameter example for a fully sharded (\\(W=F=16)\\) FSDP unit, consisting of one \\(4 \\times 3\\) nn.Linear layer. Figure from [7].For an \\(M\\)-size model split into \\(K\\) units with sizes \\(M_1, \\ldots, M_K\\), where \\(\\sum_{i=1}^K M_i=M\\), the maximum memory usage is in \\(O(\\frac{M}{F} + \\max_{i=1}^K M_i)\\).\nMore precisely, it is the sum of the sharded parameters, gradients and optimizer state, combined with the largest unsharded unit’s parameters and gradients (but not the more expensive optimizer state, which always remains sharded).\nConversely, even though total communication is not affected by the number of units, the number of collectives over which it is spread is \\(O(K)\\).\nTherefore the number of units presents yet another memory-communication tradeoff.\nPyTorch lets the user control this with the auto_wrap_policy argument to FSDP, or by manually wrapping individual submodules rather than a single wrapper around the entire model13.\nAs with DDP’s Gradient Bucketing, FSDP tries to overlap communication and computation as much as possible.\nHere’s what that looks like for our previous three unit, six layer example:\n\nFSDP, like DDP, is implemented by registering forward/backward hooks as well as overriding the forward() method.\nFull forward & backward pass for previous 3 unit, 6 layer FSDP example. Compute & communication CUDA streams (below), and broken up by unit (above).In DDP’s backward pass, we were able to compute gradients and then asynchronously AllReduce them afterwards.\nThis isn’t possible for FSDP’s forward: we need to AllGather parameters before computing, and (because of eager execution) we don’t know which FlatParameter to gather next – thus we can’t reorder the async AllGather of the next unit before the synchronous computation of the current unit.\nThe solution, implicit forward prefetching (always enabled), is to use a separate stream (queue of device instructions) for communication, bypassing the false dependency on the default compute stream.\nYou may have noticed the poor compute-communication overlap in the backward pass: the ReduceScatter for the current unit blocks the AllGather for the next, which in turn blocks the next gradient computation14.\nExplicit backward prefetching issues the AllGather for the next unit before the ReduceScatter for the current one.\nTo know which FlatParameter to gather next, FSDP records the reverse forward execution order of modules each iteration.\nTwo variants exist: backward_prefetch=BACKWARD_PRE which overlaps the next AllGather with the current gradient computation, and BACKWARD_POST which waits until the current parameters are freed (using less memory but reducing overlap).\nBy default FSDP limits the rate at which prefetch AllGathers are issued to ensure memory usage of at most two consecutive units.\nFSDP makes one final optimisation: it assumes the root unit (wrapping the outermost module) holds the last layer’s parameters, and does not free the root unit’s parameters after the forward pass (with the intention that they are immediately re-used for backward).\nBecause this naively sidesteps eager execution, it doesn’t always work.\nIn our example, it’s actually unit 3 that holds the last layer and we end up AllGathering parameters we already have and are about to free!\nLastly, we should note that with hybrid sharding there would also be an async AllReduce (on yet another communication stream) for each unit after their ReduceScatter is done.\nPyTorch’s FSDP experiments [7] show near-linear compute scaling, though this regresses substantially for larger clusters where “a near-perfect overlap between communication and computation is no longer attainable”:\nFully-sharded training of the T5-11B transformer; TFLOPS per GPU for batchsizes 8 and 16; A100 80GB GPUs with 2Tb/s RoCE interconnects. Figure from [7].Pipeline Parallel (PP)\n[Further Reading]\nLike FSDP, pipeline parallelism aims to train models too large to fit within a single GPU.\nHowever, rather than sharding the model horizontally, we partition it vertically along its depth.\nEach partition of consecutive layers is referred to as a stage.\nReturning to our four layer network from Section 1, we could partition it evenly across two ranks and send intermediate activations/gradients at partition boundaries between stages:\nPebble graph of a four layer network, partitioned across two ranks into two stages of two layers each (image credit).This naive approach of passing a single batch from rank to rank (often referred to as “model parallelism”), results in severe GPU under-utilisation: only one GPU works on the batch at any given moment, so each rank is busy at most \\(\\frac{1}{W}\\) of the time.\nTo illustrate, here’s what the same naive schedule would look like with a pipeline depth of four stages:\nNaive model parallelism with d=4 stages; FWD/BWD are over entire stages rather than only a single layer.These dead zones in our schedule where GPUs are idle are called pipeline bubbles.\nThey are caused by dependencies between operations: for example, rank 2 cannot start the 2nd forward stage until it has received 1st stage intermediate outputs from rank 1.\nGPipe[8] reduces bubbles by splitting a batch into microbatches and adding up each of their gradients to get back the gradient over the entire batch (the same as DDP gradient accumulation), thus allowing more than one rank to do useful work at the same time.\nHere’s the same four stage example, with 4-way batch-splitting:\nGPipe schedule with \\(d=4\\) stages, \\(m=4\\) microbatches; \\(Fi(j)\\) denotes the \\(i^\\text{th}\\) stage forward computation over the \\(j^\\text{th}\\) microbatch.We can show empirically that, in this example, GPipe reduces relative bubble time by more than two thirds.\nFor a pipeline with \\(d\\) evenly-partitioned stages and \\(m\\) evenly-divided microbatches, a given stage spends \\(m\\) forward timesteps doing useful work and \\(d-1\\) forward timesteps waiting for new work to arrive during the forward pass.\nThe same applies to the backward pass, therefore the overall fraction of ideal computation time spent in bubbles is:\n\\[\n\\text{Bubble}(d,m)=\\frac{d-1}{m}\n\\]\nFor the bubble time fraction to be small we need \\(m \\gg d\\).\nOur naive model parallelism example (\\(m=1, d=4\\)) has a bubble ratio of \\(3\\), compared to \\(0.75\\) for GPipe (\\(m=4, d=4\\)).\nSo far we’ve ignored communication overheads.\nUnlike other parallelisation paradigms, pipelining does not require any collective communication primitives; we simply asynchronously send (p2p) intermediates as soon as they’re ready.\nHere’s what our GPipe example looks like once we include communication:\nGPipe schedule with \\(d=4\\) stages and \\(m=4\\) microbatches, communication included.For illustration purposes, here sending a microbatch takes longer than computing a stage (pipelining is typically internode so this is not uncommon), reducing our compute efficiency.\nPerfect compute-communication overlap is impossible for pipeline parallelism because necessarily we can’t start working on the first microbatch until the previous stage has finished processing and then sent the same microbatch.\nNotably, pipeline parallelism is orthogonal to DDP and both can be combined to obtain a 2D parallelism similar to hybrid FSDP.\nIn practice, this is implemented with the pipeline as the inner dimension and with bucketed AllReduces in the outer dimension (interleaved with the backward pass of the final microbatch).\nDDP and pipeline 2D parallelism example over \\(W=4\\) ranks, \\(d=2\\) stages (image credit).Activation Checkpointing\n[Further Reading]\nWhile we’ve discussed ways of reducing memory demand, you may have spotted another easy target: activations.\nWith GPipe, stages need to cache activations for each microbatch from the start of its forward to the end of its corresponding backward.\nFor an \\(\\ell\\) layer network (assuming each layer is roughly equal size) with batchsize \\(B\\), the peak per-stage memory demand for caching activations is15:\n\\[\nO\\left(B \\frac{\\ell}{d}\\right)\n\\]\nWith activation checkpointing16 (aka gradient checkpointing) [9], we only store boundary activations and recompute the forward for each microbatch when it’s time to do its backward.\nBoundary activations take \\(O(B)\\) space and we only need to cache activations for a single microbatch at any given moment (while computing its gradient), reducing peak memory demand to:\n\\[\nO\\left(B+\\frac{B}{m}\\frac{l}{d}\\right)\n\\]\nWhy can we get away with recomputing the forward without significantly impacting overall compute efficiency17?\nIn practice the backward is actually much more expensive (usually twice as much) than the forward.\nAs we saw earlier pipeline parallelism can’t achieve perfect compute-communication overlap. When used in conjunction with pipelining, forward recomputation can be scheduled earlier (during bubbles) as we don’t need to wait for the gradients from later layers.\nThe original GPipe paper [8] claims that, with activation checkpointing, using \\(m \\geq 4d\\) microbatches results in “negligible” bubble overhead.\nOther Schedules\n[Further Reading]\nWe can further decrease memory demand by reducing the number of “in-flight” microbatches for which we need to cache activations (or checkpoints).\nIf you look at the original GPipe schedule, after completing the forward for the first microbatch, the last stage could instead start the backward pass right away and then discard its activation.\nPipeDream[11] schedules the last stage backward immediately after the corresponding forward, reducing memory demand compared to GPipe:\n1F1B PipeDream-flush schedule, ignoring communication with \\(d=4, m=8\\). Based on figure from [12].The schedule consists of three phases:\nThe warmup where deeper stages are waiting on activations from earlier stages. We limit the number of contiguous microbatches over which we compute a forward pass to the pipeline depth, thus also limiting the number of in-flight microbatches.\nThe steady state where ranks perform one forward pass followed by one backward pass (known as “1F1B”).\nLastly we flush the pipeline by completing the backward passes for remaining microbatches without scheduling any new ones.\nUnlike GPipe, where all microbatches are in-flight at some point during the schedule, with PipeDream that number never exceeds the pipeline depth.\nSince reducing bubble time requires \\(m \\gg d\\), PipeDream can do so without affecting memory footprint.\nHowever, for fixed \\(m\\), the bubble fraction is no different between PipeDream and GPipe (you can see this by shifting all the blue forwarded passes left).\nSo how can we do better, without prohibitively increasing \\(m\\) and thus the overall batchsize?\nRather than each rank only having a single stage (of consecutive layers), we can loop our pipeline by performing the computation for \\(v\\) (non-consecutive) stages at each rank and connecting the last rank to the first, forming a coil.\nFor example, if rank 0 previously had 4 layers (e.g. layers 0-4), with \\(v=2\\) loops it would now have 2 stages of 2 layers each (e.g. layers 0-1 and 8-9).\nInterleaved 1F1B[12] is the looped version of the PipeDream schedule:\nInterleaved 1F1B (DFS) schedule; \\(W=4\\) ranks, \\(d=16\\) stages, \\(v=4\\) loops, \\(m=8\\) microbatches. Data-parallel AllReduce illustrated on odd rows, pipeline-parallel communication omitted. Figure from [13].Forward and backward passes for a stage are now shorter by a factor of \\(v\\), so the bubble time at each rank is also reduced by \\(v\\) to \\((d-1)/v\\).\nThe overall bubble fraction becomes:\n\\[ \\text{Bubble}_\\text{looped}(d,m,v) = \\frac{d-1}{vm} \\]\nThis reduced bubble size does not, however, come for free: the total communication volume is increased by the same factor of \\(v\\).\nAnother caveat is that Interleaved 1F1B requires \\(m\\) be a multiple of \\(W\\).\nYou can imagine that 1F1B schedules like PipeDream and Interleaved 1F1B are depth-first: when deciding between computing the same (forward) stage for the next microbatch or the next (backward) stage for the same microbatch, a rank will choose the latter – sending earlier microbatches as deep as possible down the pipeline.\n“All-forward all-backward” schedules like GPipe are breadth-first: a rank will prioritise completing all microbatches in the earliest unfinished stage.\nNaturally, you might ask what the looped anolog of GPipe looks like.\nThis is the breadth-first pipeline schedule (BFS) [13]:\nBreadth-first pipeline schedule; \\(W=4\\) ranks, \\(d=16\\) stages, \\(v=4\\) loops, \\(m=8\\) microbatches. Data-parallel AllReduce illustrated on odd rows, pipeline-parallel communication omitted. Notice that a complete iteration finishes faster than Interleaved 1F1B. Figure from [13].The bubble time fraction is exactly the same as Interleaved 1F1B (“DFS” from now on), though (like GPipe) peak memory consumption is increased as all microbatches are in-flight at some point.\nHowever, BFS achieves much better communication overlap:\nA DFS rank will compute the forward over \\(d\\) contiguous microbatches at a time; the first microbatch must complete the rest of the loop before the sequence is finished (i.e. in \\(<d-1\\) fwd timesteps) to avoid starving the rank (impossible with non-zero communication delay). BFS, on the other hands, has \\(m-d\\) extra microbatches to absorb communication overheads because it forwards the entire batch.\nThe backward for the first stages finishes earlier, allowing us to start reducing gradients along the DP dimension earlier.\nMore importantly, BFS can make use of FSDP.\nIn a non-looping pipeline, a rank only has a single stage. To achieve any memory savings, FSDP units would have to be intra-stage thus requiring a complete FSDP iteration for each microbatch.\nWith a looping pipeline, ranks hold several stages each of which can be used as a unit; we can keep a stage unsharded and accumulate gradeints for contiguous microbatches, only resharding/reducing at the end of the microbatch sequence.\nA BFS stage completes the forward (or backward) pass for all microbatches in one go, avoiding the repeated FSDP iterations that arise from alternating between stages in DFS.\nThe benefits of combining BFS and FSDP are two-fold: first, FSDP compensates for BFS’ larger memory footprint by sharding stages; second, BFS reduces the size of FSDP sharding groups (and their expensive collectives) while maintaining the same overall degree of parallelism.\nLike DDP, FSDP is typically the outermost parallelism when combined with pipelining.\nOuter dimensions may spread across a multi-hop network with higher communication latency and lower bandwidth; FSDP has fewer, larger asynchronous collectives that can better absorb these communication delays.\n\n\nThere are a lot other pipeline schedules we haven’t covered here, a good reference is the PyTorch documentation.\nMore advanced scheduling strategies, such as those used to train Llama 3 [1], include hybrid schedules that combine the memory savings of DFS with the communication efficiency of BFS.\n\nPP in PyTorch\n[WIP]\n\n\nTensor Parallel (TP)\n[Further Reading]\nTensor parallelism is similar to FSDP: we split our model horizontally to reduce memory footprint and increase our degree of parallelism.\nHowever, TP shards are much more granular, with splits within a single layer rather than across units of several layers.\nThe computational bottleneck for most modern models (such as the transformer) is the general matrix multiply (GEMM): multiplying an activation batch matrix \\(X\\) with a large weight matrix \\(A\\).\nThe example we used in Section 1, a multi-layer perceptron (MLP), consists of a GEMM followed by a pointwise nonlinear activation function \\(\\sigma(\\cdot)\\):\n\\[\nY = \\sigma(XA)\n\\]\nOne way to paralellise the GEMM would be to split the weight matrix \\(A\\) along its rows, and input \\(X\\) along its columns:\n\\[\nX = \\begin{bmatrix} X_1 & X_2 \\end{bmatrix}, A= \\begin{bmatrix} A_1 \\\\ A_2 \\end{bmatrix}\n\\]\nMatrix multiplication can be thought of as a dot product between pairs of rows (on the left) and columns (on the right).\nIt’s therefore possible to compute independent dot products on different ranks and sum up the results:\n\\[\nY=X_1A_1+X_2A_2\n\\]\nHowever, because \\(\\sigma\\) is a nonlinear, in general \\(\\sigma(X_1 A_1 + X_2A_2) \\neq \\sigma(X_1 A_1) + \\sigma(X_2 A_2)\\).\nThis approach would therefore require a synchronisation point before the activation function.\nIf instead we only split weights along their columns \\(A=\\begin{bmatrix} A_1 & A_2 \\end{bmatrix}\\), we end up concatenating rather than adding the outputs:\n\\[\nY=\\begin{bmatrix} Y_1 & Y_2 \\end{bmatrix}=\\begin{bmatrix} \\sigma(X A_1) & \\sigma(X A_2) \\end{bmatrix}\n\\]\nBy removing the pre-activation synchronisation, we can stack a second MLP layer before we have to synchronise again:\n\\[\nZ = \\sigma_1(YB)=\\sigma_1(Y_1B_1 + Y_2B_2)\n\\]\nThe second layer weights are split column-wise, \\(B= [B_1; B_2]\\), and we synchronise before the second activation function \\(\\sigma_1\\) (as in our original attempt).\nWe end up sharding our two MLP layers with a single AllReduce synchronisation before the final activation:\nTensor parallelism over \\(W=2\\) rank applied to two consecutive MLP layers. Here \\(\\sigma_0\\) and \\(\\sigma_1\\) are GeLU and dropout functions respectively. \\(f\\) is the identify and \\(g\\) is an AllReduce. Figure from [14]Because TP requires an AllReduce per layer (or every two layers), it is almost exclusively used intranode.\nYou can think of it as agglomerating the ranks in a node into a single, much larger rank (over which we can apply other parallelisms).\n\nTP in PyTorch\n[WIP]\nContext Parallel (CP)\n[WIP]\n\nCP in PyTorch\n\n4. Parallelism in Practice\n[WIP] Large models use FSDP, gigantic models use all of the above.\n\n\n\n\n1. A. @. M. Llama Team, The llama 3 herd of models. (2024).\n\n\n2. D. P. Kingma & J. Ba, Adam: A method for stochastic optimization. (2017).\n\n\n3. P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, & H. Wu, Mixed precision training. (2018).\n\n\n4. S. McCandlish, J. Kaplan, D. Amodei, & O. D. Team, An empirical model of large-batch training. (2018).\n\n\n5. S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke, J. Smith, B. Vaughan, P. Damania, & S. Chintala, PyTorch distributed: Experiences on accelerating data parallel training. (2020).\n\n\n6. S. Rajbhandari, J. Rasley, O. Ruwase, & Y. He, ZeRO: Memory optimizations toward training trillion parameter models. (2020).\n\n\n7. Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, P. Damania, B. Nguyen, G. Chauhan, Y. Hao, A. Mathews, & S. Li, PyTorch FSDP: Experiences on scaling fully sharded data parallel. (2023).\n\n\n8. Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, & Z. Chen, GPipe: Efficient training of giant neural networks using pipeline parallelism. (2019).\n\n\n9. T. Chen, B. Xu, C. Zhang, & C. Guestrin, Training deep nets with sublinear memory cost. (2016).\n\n\n10. V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, & B. Catanzaro, Reducing activation recomputation in large transformer models. In D. Song, M. Carbin, & T. Chen,eds., Proceedings of machine learning and systems (Curan, 2023), pp. 341–353.\n\n\n11. A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, & P. Gibbons, PipeDream: Fast and efficient pipeline parallel DNN training. (2018).\n\n\n12. D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. A. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, & M. Zaharia, Efficient large-scale language model training on GPU clusters using megatron-LM. (2021).\n\n\n13. J. Lamy-Poirier, Breadth-first pipeline parallelism. (2023).\n\n\n14. M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, & B. Catanzaro, Megatron-LM: Training multi-billion parameter language models using model parallelism. (2020).\n\n\nIn particular we’re referring to the pre-training phase for LLMs.↩︎\nIn practice an epoch will loop over an entire training set consisting of several batches (each with their own parameter updates), potentially followed by evaluation on separate validation batches.↩︎\nSome tensors in a module don’t have gradients, for example fixed transforms with static parameters.↩︎\nFP16 weights and gradients + FP32 master copy of weights + FP32 momentum and variance.↩︎\nFloating point addition is not associative, but in practice the difference is small enough to be safely ignored.↩︎\nExcept for batch-wise operations like BatchNorm, which won’t be locally consistent (unless we use their expensive synchronised implementations like SyncBatchNorm).↩︎\nA batch is used to approximate the gradient over the entire training set; for large batches the approximation is already very good, and further increasing batchsize provides negligible improvement (i.e. no longer reducing number of training steps), wasting compute. Moreover, large batchsizes can harm out-of-sample performance by reducing stochasticity.↩︎\nBucket size is user-configurable. Larger buckets lower communication overhead but reduce overlap with compute. Buckets are allocated heuristically during model construction, by the reverse order of model.parameters().↩︎\nNote that because the optimizer step will only operate on the sharded parameters, any optimizer that depends on global state over all parameters won’t be locally consistent.↩︎\nThe actual class is distributed.fsdp.FullyShardedDataParallel. Note that the optimizer should be initialised afterwards, using the sharded module.↩︎\nThere’s also NO_GRAD_OP which keeps parameters unsharded during the entire forward-backward computation.↩︎\nBefore forward computation, FSDP replaces the original parameters with views into their unsharded FlatParameter so that autograd behaves correctly. Keeping the original parameters registered requires using the recently added use_orig_params flag.↩︎\ne.g. for a transformer model we’ll usually wrap each transformer block, with a final wrapper around the root module sharding the initial embedding and final linear layers.↩︎\nWe can’t get around this with an extra stream, PyTorch only uses one internal NCCL stream for a given process group.↩︎\nA stage has \\(\\frac{l}{d}\\) layers, each of which caches \\(O(B)\\) of activations.↩︎\nNot to be confused with model checkpointing, where we periodically save the entire model to disk, usually at the end of an epoch.↩︎\nRecomputing activations will still slow training speed; instead the SoTA is to selectively recompute only some activations [10].↩︎\n",
    "preview": {},
    "last_modified": "2025-01-16T09:19:31+01:00",
    "input_file": {}
  },
  {
    "path": "posts/boosting-for-causal-effect-estimation/",
    "title": "Boosting for Causal Effect Estimation",
    "description": "Can you boost when you don't have labels?",
    "author": [
      {
        "name": "Bruce Mauger",
        "url": {}
      }
    ],
    "date": "2024-01-27",
    "categories": [],
    "contents": "\n\nContents\nBackground\nUplift Trees\nBoosting\nBackground\nAdaBoost\n\nBoosting Uplift Trees\nConclusion\nProof of (\\(\\ref{eq:cateuplift}\\))\nTree Splitting Criterion (\\(\\ref{eq:utobjective}\\))\n\n\nThis post provides a very brief overview of my own, original adaptation\nof the AdaBoost algorithm to the problem of (heterogeneous) causal\neffect estimation as part of my undergraduate thesis on causal machine learning.\nIt also includes a light introduction to boosting and\nuplift modelling, though a basic knowledge of probabilities and decision\ntrees is assumed.\nBackground\nUplift modelling is a branch of machine learning that aims to predict\nthe causal effect of an action (“treatment”) on a given individual\n[1]. To illustrate, consider a marketing campaign where\nindividuals either do or don’t receive a particular ad (the treatment),\nand the outcome is whether they made a purchase. The goal is to select\nindividuals most likely to respond positively to the campaign. Causal\neffect estimation has also seen use in many fields outside of marketing,\nincluding medicine, A/B testing and econometrics.\nWe consider a formalism of the problem in terms of the potential\noutcomes framework [2]. Given a dataset of \\(N\\) independent and\nidentically distributed \\((X_i, W_i, Y_i)\\) where \\(X_i \\in \\mathcal{X}\\) are\nper-individual features, while \\(W_i \\in \\{0, 1\\}\\) and \\(Y_i \\in \\{0, 1\\}\\)\ndenote the individual’s treatment assignment and outcome respectively.\nLet \\(\\{Y_i(0), Y_i(1)\\}\\) be the potential outcomes we would have\nobserved had individual \\(i\\) be assigned treatment \\(W_i=0\\) or \\(1\\)\nrespectively. The causal effect of treatment on individual \\(i\\) is the\nIndividual Treatment Effect (ITE), defined as:\n\\[\\tau_i := Y_i(1) - Y_i(0)\\]\nUnfortunately, ITE is not identifiable as we cannot simultaneously\nobserve both potential outcomes: we only observe \\(Y_i=Y_i(W_i)\\) (this is\noften referred to as the Fundamental Problem of Causal Inference).\nInstead, we seek to estimate the conditional average treatment effect\n(CATE):\n\\[\\tau(\\mathbf x) := \\mathbb E[Y(1) - Y(0) \\mid X=\\mathbf x]\\]\n\nAssumption 1 (Unconfoundedness). Potential outcomes are independent of treatment assignment:\n\\(\\{Y_i(0), Y_i(1)\\} \\perp W_i \\mid X_i\\)\n\nIn order to identify the CATE, we assume unconfoundedness (Assumption\n1) which yields the result (proof in the Appendix):\n\\[\\begin{align}\n\\tau(\\mathbf x) &= \\mathbb E[Y(1) \\mid X=\\mathbf x] - \\mathbb E[Y(0) \\mid X=\\mathbf x]  \\nonumber \\\\\n    &= \\mathbb E[Y \\mid W=1, X=\\mathbf x] - \\mathbb E[Y \\mid W=0, X=\\mathbf x] \\label{eq:cateuplift}\n\\end{align}\\]\nwhich is often referred to as the uplift. Unconfoundedness means that\n\\(W_i\\) only affects which of the potential outcomes\n\\(\\{Y_i(0), Y_i(1)\\}\\) is observed, without affecting how they are each\ngenerated.\nThe uplift literature will often make another, stronger assumption that\nthe data originates from a randomised control trial (RCT):\n\nAssumption 2 (Randomised Treatment Assignment).\n\\(e(X_i) = 0.5 = (1-e(X_i))\\)\n\nwhere we write \\(e(X_i) = \\Pr(W_i = 1 \\mid X_i)\\) for the treatment\npropensity. That is, each individual is equally likely to be assigned to the treatment or control group.\nUplift Trees\nThe straightforward way to model uplift is to estimate\n\\(\\mathbb E[Y| W=1, X=\\mathbf x]\\) and \\(\\mathbb E[Y| W=1, X=\\mathbf x]\\)\nseparately. However, since it is not the absolute values of the\nresponses, but the differences in potential outcomes that matter when\nestimating treatment effect, modelling \\(\\tau(\\mathbf x)\\) directly can\nproduce better results.\nTo that end, Rzepakowski and Jaroszewicz propose a modified decision\ntree for direct uplift estimation [3]. Their procedure is\nsimilar to fitting a normal decision tree, in that the model is built by\ngreedily partitioning \\(\\mathcal X\\) into regions \\(R_1, \\ldots, R_J\\) (each\nof which corresponds to a leaf node in the tree) according to a\nsplitting criterion.\nSince the more ubiquitous splitting criteria based on maximising class\nhomogeneity don’t apply (recall: we can’t observe \\(\\tau_i\\)), the authors\npropose several criteria based on maximising the divergence between the\ntreatment and control distributions \\((Y|W=1), (Y|W=0)\\). They argue that,\nof these, Euclidean distance is superior because of its symmetry and\nstability. For a binary tree with binary \\(W_i\\), this reduces to\nmaximising (further details in the Appendix):\n\\[\n\\begin{equation}\n    \\mathcal C^\\mathrm{Euclidean} :=\n\\Pr(\\mathbf x \\in R_\\mathrm{Left}) \\hat\\tau(R_\\mathrm{Left})^2 +\n\\Pr(\\mathbf x \\in R_\\mathrm{Right}) \\hat\\tau(R_\\mathrm{Right})^2 \\label{eq:utobjective} \\end{equation}\n\\]\nwhen splitting into left and right child nodes with regions\n\\(R_\\mathrm{Left}, R_\\mathrm{Right}\\). We write \\(\\hat\\tau(R_j)\\) for the\nempirical average treatment effect in region \\(R_j\\) (over our training\nset):\n\\[\n\\begin{equation}\n\\hat \\tau(R_j) := \\mathbb E[Y \\mid W=1, \\mathbf x \\in R_j] - \\mathbb E[Y \\mid W=0, \\mathbf x \\in R_j]\n\\end{equation}\n\\]\nThe prediction at each leaf node of the tree is then the empirical\naverage treatment effect in the associated region.\nExample fit of Rzepakowski and Jaroszewicz’ decision treeLastly, Rzepakowski and Jaroszewicz also propose a regularisation\ntechnique penalising splits that produce imbalanced treatment and\ncontrol groups.\nBoosting\nSome of the best performing models on classical machine learning\nproblems are those based around boosting an ensemble of weak learners\n(usually trees). Thus, a natural progression from Rzepakowski and\nJaroszewicz’ decision tree is to ask whether it can be boosted.\nBackground\nBoosting takes a basis function \\(h : \\mathcal X \\to \\mathbb R\\) and\nproduces an additive model\n\\(H_T(\\mathbf x) = \\sum_{t=1}^T \\alpha_t h_t (\\mathbf x)\\). Typically, the\nensemble is fit by minimising a (convex and differentiable) loss\nfunction \\(L\\) over the training data:\n\\[\n\\begin{equation}\n\\label{eq:boosting}\n    \\min_{\\{\\alpha_t, h_t \\}_1^T} \\sum_{i=1}^N L(y_i, \\sum_{t=1}^T \\alpha_t h_t(\\mathbf x_i))\n\\end{equation}\n\\]\nOften, this is not a feasible computation. Forward Stagewise Additive\nModelling (FSAM) approximates the solution to\n(\\(\\ref{eq:boosting}\\)) by greedily fitting each basis function. We\nstart by initialising \\(H_0(\\mathbf x) := 0\\). Then, at each iteration \\(t\\)\nwe solve for the optimal basis function \\(h_t\\) and corresponding weight\n\\(\\alpha_t\\), given the functions we have already fit:\n\\[\n\\begin{equation}\n\\label{eq:fsam}\n    h_{t+1}, \\alpha_{t+1} = \\mathrm{argmin}_{h \\in \\mathcal H, \\alpha \\in \\mathbb R^+} \\sum_{i=1}^N L(y_i, H_t(\\mathbf x_i)+\\alpha h(\\mathbf x_i))\n\\end{equation}\n\\]\nand update our ensemble: \\(H_{t+1} := H_t + \\alpha_{t+1} h_{t+1}\\).\nOne way to solve for (\\(\\ref{eq:fsam}\\)) is gradient descent in functional space. We\nwrite \\(\\ell(H_t) = \\sum_{i=1}^N L(y_i, H_t(x_i))\\) for our total loss. Fixing a\nsmall step-size \\(\\alpha\\), we can use a Taylor approximation on\n\\(\\ell(H_t + \\alpha h)\\) to find an almost optimal \\(h\\):\n\\[\n\\begin{align}\n    \\mathrm{argmin}_{h \\in \\mathcal H} \\ell (H_t + \\alpha h)\n    &\\approx \\mathrm{argmin}_{h \\in \\mathcal H} \\;\\ell (H_t) + \\alpha \\langle \\nabla \\ell (H_t), h \\rangle \\nonumber \\\\\n    &= \\mathrm{argmin}_{h \\in \\mathcal H} \\; \\langle \\nabla \\ell (H_t), h \\rangle \\nonumber\\\\\n    &= \\mathrm{argmin}_{h \\in \\mathcal H} \\sum_{i=1}^N \\frac{\\partial \\ell}{\\partial [H_t(\\mathbf x_i)]} h(\\mathbf x_i) \\label{eq:vecapprox}\\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N \\underbrace{-\\frac{\\partial \\ell}{\\partial [H_t(\\mathbf x_i)]}}_{t_i} h(\\mathbf x_i) \\label{eq:graddesc}\n\\end{align}\n\\]\nIn (\\(\\ref{eq:vecapprox}\\)), we use the fact that a function \\(f(\\cdot)\\)\nin our functional space is completely specified by its values on our\ntraining set \\(f(\\mathbf x_1), \\ldots, f(\\mathbf x_n)\\), and hence can be\nconsidered a vector \\(\\vec f \\in \\mathbb R^N\\).\nThe final result (\\(\\ref{eq:graddesc}\\)) has the intuitive interpretation of finding\nthe basis function \\(h\\) closest to the negative gradient of the loss\n\\(\\vec t\\). Note that \\(h\\) need not be perfectly optimal to make progress.\nAs long as \\(\\sum_{i=1}^N t_i h(\\mathbf x_i) > 0\\) (that is, the basis\nfunction & negative gradient lie on the same side of the hyperplane),\nour loss \\(\\ell\\) will decrease. This is the weak learner condition: our\nbasis function must be better than a random function \\(h_R\\), for which we\nwould expect \\(\\sum_{i=1}^N t_i h_R(\\textbf x_i) = 0\\).\nAdaBoost\nOne of the most popular, and earliest, boosting algorithms is the\nAdaBoost algorithm [4]. It was\nlater shown to be a special case of FSAM, with \\(y_i \\in \\{-1, 1\\}\\),\n\\(h(\\mathbf x_i) \\in [-1,1]\\) 1 and optimising for exponential loss [6]:\n\\[\n\\begin{equation}\nL(y, f(x)) = \\exp({-y f(x)})\n\\end{equation}\n\\]\nFirst, we show that \\(\\ell(H_t)\\) at each iteration is equivalent to\nminimising loss for the basis function under a re-weighted distribution:\n\\[\n\\begin{align}\n    h_{t+1}\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N t_i h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N -\\left[\\frac{\\partial}{\\partial H_t(\\mathbf x_i)} \\sum_{i=1}^N \\exp(-y_iH_t(\\mathbf x_i))\\right] h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i)) y_i h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H}\\; \\frac{1}{\\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i))} \\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i)) y_i h(\\mathbf x_i) \\nonumber \\\\\n    &= \\mathrm{argmax}_{h \\in \\mathcal H} \\mathbb E_{i \\sim D_t} [y_i h(\\mathbf x_i)] \\label{eq:adaobjective} \\\\\n\\end{align}\n\\]\nwhere\n\\(D_t(i) = \\frac{\\exp(-y_i H_t(\\mathbf x_i))}{\\sum_{j=1}^N \\exp(-y_j H_t(\\mathbf x_i))} = \\frac{1}{Z_t}\\exp(-y_i H_t(\\mathbf x_i))\\)\nis the weight associated with \\(i^\\mathrm{th}\\) training sample. Note that\nthe normalisation factor \\(Z_t\\) is identical to the total loss\n\\(\\ell(H_t)\\). Each weight \\(D_t(i)\\) can be interpreted as the relative\ncontribution of the \\(i^\\text{th}\\) training sample to the total loss.\nMoreover, in the discrete case where \\(h(\\mathbf x_i) \\in \\{-1,1\\}\\), the\nlearning objective\n(\\(\\ref{eq:adaobjective}\\)) is equivalent to maximising accuracy under\nthe re-weighted distribution.\nMost boosting algorithms do not yield a tractable solution for the\noptimal step-size \\(\\alpha_{t+1}\\) (hence \\(\\alpha\\) is often left as a\nfixed hyperparameter). However, AdaBoost is exceptional in that we can\nfind a (near) optimal step-size. A consequence is that AdaBoost\nconverges fast and overfits slowly [6].\nIn order to find the optimal \\(\\alpha_{t+1}\\), consider the FSAM\nminimisation procedure (\\(\\ref{eq:fsam}\\)):\n\\[\n\\begin{align}\n    \\alpha_{t+1}\n    &= \\mathrm{argmin}_{\\alpha \\in \\mathbb R^+} \\sum_{i=1}^N \\exp(-y_i (H_t(\\mathbf x_i) + \\alpha h(\\mathbf x_i)))  \\nonumber\\\\\n    &= \\mathrm{argmin}_{\\alpha \\in \\mathbb R^+} \\sum_{i=1}^N \\exp(-y_i H_t(\\mathbf x_i)) \\exp(-\\alpha y_i h(\\mathbf x_i)) \\nonumber\\\\\n    &= \\mathrm{argmin}_{\\alpha \\in \\mathbb R^+} \\mathbb E_{i \\sim D_t} [\\exp(-\\alpha y_i h(\\mathbf x_i))] \\label{eq:alphaobjective} \\\\\n\\end{align}\n\\]\nSchapire and Singer propose an upper bound on the objective\n(\\(\\ref{eq:alphaobjective}\\)):\n\\[\n\\begin{equation}\n\\mathbb E_{i \\sim D_t}[\\exp(-\\alpha y_i h(\\mathbf x_i))] \\leq \\mathbb E_{i \\sim D_t}\\left[\\frac{1+y_i h(\\mathbf x_i)}{2} e^{-\\alpha} + \\frac{1-y_i h(\\mathbf x_i)}{2} e^{\\alpha}\\right]\n\\end{equation}\n\\]\nthis upper bound is valid since \\(y_i h(\\mathbf x_i) \\in [-1, 1]\\) 2. The\nstep-size minimising the upper bound can be found analytically, giving:\n\\[\n\\begin{equation}\\alpha_{t+1} = \\frac{1}{2} \\ln (\\frac{1+r_{t+1}}{1-{r_{t+1}}})\\end{equation}\n\\]\nwhere \\(r_{t+1} = \\mathbb E_{i \\sim D_t} [y_i h(\\mathbf x_i)]\\).\nBoosting Uplift Trees\nWe now return to our original question: is it possible to apply a\nboosting algorithm (in particular, the AdaBoost algorithm) to\nRzepakowski and Jaroszewicz’ uplift decision tree? Once again, the\nFundamental Problem of Causal Inference adds a difficulty: there is no\nobvious labelling to use as we cannot observe the ground truth \\(\\tau_i\\).\nLet us consider the following simple class transformation:\n\\[\n\\begin{equation}\n\\hat Y_i :=\n    \\begin{cases}\n        +1 & \\mathrm{if}\\; W_i = Y_i \\\\\n        -1 & \\mathrm{otherwise}\\; (W_i \\neq Y_i)\n    \\end{cases}\n\\end{equation}\n\\]\nWe naïvely assume there is a positive causal effect\nfor positive outcomes in the treatment group or negative outcomes in the\ncontrol group, and a negative causal effect otherwise.\nSince \\(\\hat Y_i \\in \\{-1, 1\\}\\), it is a suitable label for AdaBoost. To\nfind the function that would be estimated by the model, we first examine\nthe exponential loss population minimiser:\n\\[\n\\begin{equation}\nf^*(x) = \\mathrm{argmin}_{f(x)}\\mathbb E_{Y \\mid x}\\exp{(-Y f(x))} = \\frac{1}{2} \\log \\frac{\\Pr(Y=1 \\mid x)}{\\Pr(Y=-1 \\mid x)}\n\\end{equation}\n\\]\nwhich can easily be found analytically.\nThe relevant probability for \\(\\hat Y\\) is:\n\\[\n\\begin{align}\n    \\Pr(\\hat Y = 1 | X = \\mathbf x)\n    ={}& e(\\mathbf x)\\Pr(Y = 1 | W=1, X = \\mathbf x) \\nonumber \\\\\n    &+ (1-e(\\mathbf x))\\Pr(Y = 0 | W=0, X=\\mathbf x) \\nonumber \\\\\n    ={}& e(\\mathbf x)\\mathbb E[Y \\mid W=1, X=\\mathbf x] \\nonumber \\\\\n    &+ (1-e(\\mathbf x))(1-\\mathbb E[Y \\mid W=0, X=\\mathbf x])\n\\end{align}\n\\]\nIf we assume random treatment assignment (Assumption\n2), this gives:\n\\[\n\\begin{align}\n    \\Pr (\\hat Y = 1 | X = \\mathbf x)\n    &= \\frac{1}{2}\\mathbb E[Y \\mid W=1, X=\\mathbf x] + \\frac{1}{2}(1-\\mathbb E[Y \\mid W=0, X=\\mathbf x]) \\nonumber \\\\\n    &= \\frac{1}{2} + \\frac{1}{2} \\tau (\\mathbf x)\n\\end{align}\n\\]\nIn fact, under random treatment assignment \\(\\hat Y\\) is\nan unbiased estimator of \\(\\tau(\\mathbf x)\\):\n\\[\n\\begin{align}\n    \\mathbb E[\\hat Y = 1 | X= \\mathbf x]\n    &= (\\frac{1}{2} + \\frac{1}{2}\\tau(\\mathbf x)) - (\\frac{1}{2} - \\frac{1}{2}\\tau(\\mathbf x)) \\nonumber \\\\\n    &= \\tau(\\mathbf x)\n\\end{align}\n\\]\nHence, AdaBoost with labels \\(\\hat Y_i\\) yields the additive model:\n\\[\n\\begin{align}\n    H_T(\\mathbf x)\n    &\\approx \\mathrm{argmin}_{\\{\\alpha_t, h_t\\}_1^T} \\mathbb E_{\\hat Y \\mid \\mathbf x}\\exp\\left ({-\\hat Y \\left [\\sum_{t=1}^T \\alpha_t h_t(\\mathbf x)\\right ]} \\right ) \\nonumber \\\\\n    &=\\frac{1}{2} \\log \\frac{\\Pr(\\hat Y = 1 |X=\\mathbf x)}{\\Pr(\\hat Y = -1 \\mid X=\\mathbf x)} \\nonumber \\\\\n    &=\\frac{1}{2} \\log \\frac{1+ \\tau(\\mathbf x)}{1 - \\tau(\\mathbf x)}\n\\end{align}\n\\]\nWhich with a simple transformation gives us the desired\nestimator:\n\\[\n\\begin{equation}\n\\hat H_T(\\mathbf x) := 2 \\left[\\frac{1}{1+e^{-2 H_T(\\mathbf x)}}\\right] - 1 \\approx \\tau(\\mathbf x)\n\\end{equation}\n\\]\nHaving demonstrated that AdaBoost with labels \\(\\hat Y_i\\) can directly\nmodel uplift, we now show that Rzepakowski and Jaroszewicz’ uplift\ndecision tree is a suitable weak learner.\nThe first requirement, that \\(h(\\mathbf x_i) \\in [-1, 1]\\), is trivially\nsatisfied: for any leaf node \\(R_j\\) we have \\(\\hat \\tau(R_j) \\in [-1, 1]\\).\nNext, we show that, after re-weighting, the basis function objective\n(\\(\\ref{eq:adaobjective}\\)) with labels \\(\\hat Y\\) is approximately the\nsame as the decision tree fitting objective\n(\\(\\ref{eq:utobjective}\\)):\n\\[\n\\begin{align}\n    \\mathbb E_{i \\sim D_t}[\\hat Y_i h(\\mathbf x_i)]\n    &= \\sum_{j=1}^J \\mathbb E_{i \\sim D_t}[\\hat Y_i h(\\mathbf x_i) | \\mathbf x_i \\in R_j] \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j) \\label{eq:sumnodes}\\\\\n    &= \\sum_{j=1}^J \\hat \\tau(R_j) \\mathbb E_{i \\sim D_t}[\\hat Y_i | \\mathbf x_i \\in R_j] \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j) \\nonumber \\\\\n    &= \\sum_{j=1}^J \\hat \\tau(R_j) \\mathbb E_{i \\sim D_t} [\\tau(\\mathbf x_i) | \\mathbf x_i \\in R_j] \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j) \\label{ass:objrct} \\\\\n    &= \\sum_{j=1}^J \\hat \\tau(R_j)^2 \\Pr_{i \\sim D_t}(\\mathbf x_i \\in R_j)\n\\end{align}\n\\]\nwhere in (\\(\\ref{eq:sumnodes}\\)) we apply the law of total probability to sum\nover the tree’s leaf nodes.\nThe reasons why the tree only approximates the objective\n(\\(\\ref{eq:adaobjective}\\)) are twofold. First, splits are chosen\ngreedily. Second, (\\(\\ref{ass:objrct}\\)) relies on (Assumption 2). Even if it holds\non the dataset (the root node), it may not hold within child nodes.\nHowever, the regularisation proposed by Rzepakowski and Jaroszewicz can\nalleviate this. Moreover, as previously discussed we only require that\nour basis function be better than random, thus in practice the tree\nremains suitable.\n\nAlgorithm Uplift AdaBoostInput: training set \\(\\{(\\mathbf x_i, w_i, y_i)\\}\\), number of iterations \\(T\\)\nSet boosting labels \\(\\hat y_i = w_i(2y_i - 1) + (1-w_i)(1 - 2y_i)\\)\nInitialise weights \\(D_1(i) = \\frac{1}{N}, i = 1, \\ldots, N\\)\nFor \\(t=1\\) to \\(T\\):\nFit uplift tree \\(h_t\\) with splitting criterion \\(\\mathcal C^\\text{Euclidean}\\) to the training set using weights \\(D_t(i)\\)\nCompute \\(r_t = \\sum_{i=1}^N D_t(i) \\hat y_i h(\\mathbf x_i)\\)\nCompute step-size \\(\\alpha_t = \\frac{1}{2} \\ln(\\frac{1+r_t}{1 - r_t})\\)\n\nUpdate weights \\(D_{t+1}(i) = \\frac{1}{Z_t} D_t(i) \\exp(-\\alpha_t \\hat y_i h_t(\\mathbf x_i))\\)\nOutput ensemble \\(\\hat H_T(\\mathbf x) = 2\\left( {1+\\exp({-2[\\sum_{t=1}^T \\alpha_t h_t(\\mathbf x)]})} \\right)^{-1} - 1\\)\n\nThe final algorithm is a modification of Real AdaBoost with proxy labels\n\\(\\hat y\\), and using Rzepakowski and Jaroszewicz’ uplift tree rather than\na traditional decision tree.\nConveniently, the weight initialisation (step 2) allows us to drop (Assumption 2). If we have a consistent\nestimator of propensity \\(\\hat e(\\mathbf x)\\), we can instead choose\nweights \\(D_1(i) = \\frac{1}{Z}(1/\\hat e(\\mathbf x_i))\\) and \\(D_1(i) = \\frac{1}{Z}(1/(1-\\hat e(\\mathbf x_i)))\\) for treatment and control samples respectively. Under this new distribution (Assumption\n2) holds.\nConclusion\nAfter a brief introduction to uplift modelling, we have shown that it is\ntheoretically possible to boost Rzepakowski and Jaroszewicz’ uplift tree\nwithout observing the ground truths \\(\\tau_i\\). This is achieved using a\nsurprisingly simple and naïve class transformation \\(\\hat Y_i\\). In the\nnext part of this series, we will examine the modified boosting\nalgorithm’s performance, and discuss some of the challenges that come\nwith producing useful evaluation metrics without access to the ground\ntruths.\nProof of (\\(\\ref{eq:cateuplift}\\))\nBy definition:\n\\[\n\\begin{aligned}\n    \\tau(\\mathbf x) ={}& \\mathbb E[Y(1) - Y(0) | X = \\mathbf x] \\\\\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x]e(\\mathbf x) + \\mathbb E[Y(1) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & - \\mathbb E[Y(0) | W=1, X=\\mathbf x]e(\\mathbf x) - \\mathbb E[Y(0) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x]e(\\mathbf x) + \\mathbb E[Y(1) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & - \\mathbb E[Y(0) | W=1, X=\\mathbf x]e(\\mathbf x) - \\mathbb E[Y(0) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & + \\mathbb E[Y(1) | W=1, X=\\mathbf x](1-e(\\mathbf x)) - \\mathbb E[Y(1) | W=1, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & + \\mathbb E[Y(0) | W=0, X=\\mathbf x]e(\\mathbf x) - \\mathbb E[Y(0) | W=0, X=\\mathbf x]e(\\mathbf x) \\\\\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x] - \\mathbb E[Y(0) | W=0, X=\\mathbf x](1-e(\\mathbf x)) \\\\\n    & + e(\\mathbf x)\\left( \\mathbb E[Y(0) | W = 0, X=\\mathbf x] - \\mathbb E[Y(0) | W=1, X=\\mathbf x]\\right) \\\\\n    & + (1-e(\\mathbf x)) \\left(\\mathbb E[Y(1) | W=0, X=\\mathbf x] - \\mathbb E[Y(1) | W=1, X=\\mathbf x]\\right)\n\\end{aligned}\n\\]\nUnder (Assumption 1), the observed outcome\n\\(\\mathbb E[Y(w) | W=w, X=\\mathbf x]\\) is the same as the unobserved\n\\(\\mathbb E[Y(1-w) | W=1-w, X=\\mathbf x]\\) which gives us:\n\\[\n\\begin{aligned}\n    \\tau(\\mathbf x)\n    ={}& \\mathbb E[Y(1) | W=1, X=\\mathbf x] - \\mathbb E[Y(0) | W=0, X=\\mathbf x] \\\\\n    ={}& \\mathbb E[Y | W=1, X=\\mathbf x] - \\mathbb E[Y | W=0, X=\\mathbf x]\n\\end{aligned}\n\\]\nTree Splitting Criterion (\\(\\ref{eq:utobjective}\\))\nIn their paper, Rzepakowski and Jaroszewicz argue that maximising\nEuclidean distance between treatment and control distributions is the\nbest splitting criterion. For two discrete random variables \\(P,Q\\)\nwith probabilities \\(p_i = \\Pr(P=i), q_i = \\Pr(Q=i)\\) respectively,\ntheir Euclidean distance is defined as:\n\\[D^\\mathrm{Euclidean}(P, Q) = \\sum_i (p_i - q_i)^2\\]\nRzepakowski and Jaroszewicz consider datasets with multiple treatment\ngroups, and trees with \\(n\\)-way splits. If we restrict to binary\ntreatment with binary splits, we are left maximising:\n\\[\n\\begin{aligned}\n    \\mathcal C^\\mathrm{Euclidean}  ={}& D^\\mathrm{Euclidean}((Y|W=1, \\mathbf x \\in R_\\mathrm{Left}), (Y|W=0, \\mathbf x \\in R_\\mathrm{Left})) \\Pr(\\mathbf x \\in R_\\mathrm{Left}) \\\\\n    &+ D^\\mathrm{Euclidean}((Y|W=1, \\mathbf x \\in R_\\mathrm{Left}), (Y|W=0, \\mathbf x \\in R_\\mathrm{Left})) \\Pr(\\mathbf x \\in R_\\mathrm{Left})\n\\\\\n\\end{aligned}\n\\]\nWe can show that:\n\\[\nD^\\mathrm{Euclidean}((Y|W=1, \\mathbf x \\in R_j), (Y|W=0, \\mathbf x \\in R_j)) \\\\\n\\begin{aligned}\n    \\hspace{10em}={}& [\\Pr(Y=1|W=1, \\mathbf x \\in R_j) - \\Pr(Y=1|W=0, \\mathbf x \\in R_j)]^2 \\\\\n    & + [\\Pr(Y=0|W=1, \\mathbf x \\in R_j) - \\Pr(Y=0|W=0, \\mathbf x \\in R_j)]^2 \\\\\n    ={}& [\\mathbb E[Y | W=1, \\mathbf x \\in R_j] - \\mathbb E[Y | W=0, \\mathbf x \\in R_j]]^2 \\\\\n    & + [(1-\\mathbb E[Y | W=1, \\mathbf x \\in R_j]) - (1-\\mathbb E[Y | W=1, \\mathbf x \\in R_j])]^2 \\\\\n    ={}& 2 \\hat \\tau (R_j)^2\n\\end{aligned}\n\\]\nHence:\n\\[\n\\mathcal C^\\mathrm{Euclidean} = 2 [\\hat \\tau(R_\\mathrm{Left})^2 \\Pr(\\mathbf x \\in R_\\mathrm{Left}) + \\hat\\tau(R_\\mathrm{Right})^2 \\Pr(\\mathbf x \\in R_\\mathrm{Right})]\n\\]\n(in (\\(\\ref{eq:utobjective}\\)) we drop the redundant constant).\n\n\n\n1. P. Gutierrez & J.-Y. Gérardy, Causal inference and uplift modelling: A review of the literature. In C. Hardgrove, L. Dorard, K. Thompson, & F. Douetteau,eds., Proceedings of the 3rd international conference on predictive applications and APIs (PMLR, 2017), pp. 1–13.\n\n\n2. D. Rubin, Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66 (1974). https://doi.org/10.1037/h0037350.\n\n\n3. P. Rzepakowski & S. Jaroszewicz, Decision trees for uplift modeling with single and multiple treatments. Knowledge and Information Systems, 32 (2012) 303–327.\n\n\n4. Y. Freund & R. E. Schapire, A desicion-theoretic generalization of on-line learning and an application to boosting. In P. Vitányi,ed., Computational learning theory (Berlin, Heidelberg: Springer Berlin Heidelberg, 1995), pp. 23–37.\n\n\n5. R. E. Schapire & Y. Singer, Improved boosting algorithms using confidence-rated predictions. Proceedings of the eleventh annual conference on computational learning theory (New York, NY, USA: Association for Computing Machinery, 1998), pp. 80–91. https://doi.org/10.1145/279943.279960.\n\n\n6. T. Hastie, R. Tibshirani, & J. H. Friedman, The elements of statistical learning: Data mining, inference, and prediction (Springer, 2009).\n\n\nHere we refer to the Real AdaBoost algorithm [5] that relaxes the original \\(h(\\mathbf x_i) \\in \\{-1, 1\\}\\) assumption of Discrete AdaBoost [4].↩︎\nIn fact, it is an exact bound for Discrete Adaboost since\n\\(y_i h(\\mathbf x_i) \\in \\{-1, 1\\}\\).↩︎\n",
    "preview": {},
    "last_modified": "2025-01-16T09:11:52+01:00",
    "input_file": {}
  }
]
