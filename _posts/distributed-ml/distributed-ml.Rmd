---
title: "Distributed Machine Learning: How Does it Work?"
description: |
  Distribution has become essential to training large models.
  Modern machine learning frameworks provide drop-in distribution, but few understand how these work -- and why they might be slowing down training.
author:
  - name: Bruce Mauger 
date: 12-26-2024
output:
  distill::distill_article:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"
    self_contained: false
    toc: true
citation: true
bibliography: distributed-ml.bib
csl: cambridge-university-press-numeric.csl
resources:
  exclude:
    "*.ipynb"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Learn more about creating blogs with Distill at:
# https://rstudio.github.io/distill/blog.html

```

Motivation: models & clusters are *really* big.
<!-- alternatively: you've just called torch.distribute(model), and your 100x GPU cluster is only running 50x faster than local, what's happened? --> 

We'll take a look at how parallelisation paradigms have evolved to cope with more data, more parameters and more GPUs.

I intended this as a light compilation of various sources; for more detail, I provide references to the relevant resources throughout.

[Outline of sections]
Though I intend to stay light on implementation specifics, the abstractions provided by PyTorch are nonetheless useful for illustration.
We'll be using PyTorch to illustrate and discuss implementation details. 
The first section provides a rough overview of the PyTorch model, particularly for transformers. If you're already familiar, skip to the next section.


# 1. PyTorch mental model
Pytorch is the preeminent framework, though the stuff here also (mostly) applies to other frameworks.
Useful for understanding how data flows throughout training.

### How do you train a PyTorch neural network?
[[Further Reading](https://pytorch.org/tutorials/beginner/basics/intro.html)]

PyTorch's fundamental data structure is the `Tensor`, a multi-dimensional matrix (think NumPy's `ndarray`) used to store a model's parameters and encode inputs/outputs. 
In PyTorch, a neural network is a `Module` composed by stitching other modules (layers) and functions together. 
For example, here's a simple network with two linear layers and a ReLU activation function in-between:

```{python, eval=FALSE, echo=TRUE}
from torch import nn

class NeuralNetwork(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear1 = nn.Linear(2, 4)
    self.activation_fn = nn.ReLU()
    self.linear2 = nn.Linear(4, 1)
  
  def forward(self, x):
    x = self.linear1(x)
    x = self.activation_fn(x)
    x = self.linear2(x)
    return x
```
Unsurprisingly, `forward` defines the network's forward pass: how inputs are mapped to outputs. Here a 2D input is mapped to a 1D output, with a 4D "hidden" layer. Taking the first `Linear` submodule as an example, it holds weight and bias tensors of shapes `[4,2]` and `[4]` respectively. Adding the second linear layer's parameters (the activation function doesn't have any), we can see the network has a total of 17 trainable parameters.
<!-- TODO: illustration of this network? e.g. https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html --->

This is all well and good, but we can't actually train the network yet! 
For that we need a basic training loop:

```{python, eval=FALSE, echo=TRUE}
model = NeuralNetwork() 
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
X = torch.ones(10, 2) # input batch tensor
y = torch.zeros(10, 1) # expected output (target) batch tensor

epochs = 10
for t in range(epochs): 
  # Compute prediction and loss
  pred = model(x)
  loss = torch.nn.functional.cross_entropy(pred, y)
  
  # Update parameters
  loss.backward()
  optimizer.step()
  optimizer.zero_grad()
```

We train our model for 10 epochs (iterations) over a single batch of 10 (identical) datapoints^[In practice an epoch will loop over an entire training set consisting of several batches (each with their own parameter updates), potentially followed by evaluation on separate validation batches.].
In each epoch:

1. With `model(x)`, we call the `forward` method defined earlier to obtain predictions for the entire input batch. The outputs of each layer (**"activations"**) are cached for use in the backward pass. 
2. We compute the (cross entropy) loss for these predictions and store them in the `loss` tensor.
3. We calculate the derivative of the loss with respect to each parameter with `loss.backward()`. PyTorch's autograd does this automatically by building a computational graph in the forward pass, and then applying backpropagation starting from the innermost (last) layer in the backward pass. It accumulates gradients in each tensor's `.grad` attribute^[Some tensors in a module don't have gradients, for example fixed transformers with static parameters.].
4. The optimizer defines how parameters are updated from gradients. `optimizer.step()` performs this adjustment, and `optimizer.zero_grad()` resets gradients so they don't accumulate in the next pass.

![Pebble graph for a four layer network illustrating how cached activations are built up in the forward pass, and used to calculate gradients in the backward pass ([graphic inspiration](https://siboehm.com/articles/22/data-parallel-training)).](images/pebble_graph.gif)

For each parameter in our network, we also need to store its gradient and relevant optimizer state.
The popular Adam optimizer tracks **momentum** and **variance**, exponential averages of the first and second moments respectively of each parameter's gradient [@adam2017]. 
The result is that each parameter can end up needing at least 16 bytes of memory (assuming fp16/32 mixed precision) [@zero2020]. 
For larger models such as Meta's Llama 405B that's a 6.5TB memory requirement, which makes distributing model parameters over several GPUs a necessity.

### The Transformer architecture
[[Further Reading](https://arxiv.org/pdf/1706.03762)]

The largest models trained today are transformers. 
Naturally, distributed training has evolved around the architecture, making it a valuable mental model.

# 2. Collective Communication Primitives
[[Further Reading]](https://marek.ai/allreduce-the-basis-of-multi-device-communication-for-neural-network-training.html)

<!-- what are collectives? why are we talking about them? -->
Before going into distribution strategies, we need to discuss the primitives we have available for communicating data between groups (or **"collectives"**) of GPUs. 

Let's start with a simple model: two GPUs (or **"ranks"**) with a point-to-point (p2p) connection -- this could be a fast NVLink interconnect if they're within the same host, or a slower InfiniBand or Ethernet network (perhaps with several hops) if they're not (more on this [later]()).

All collectives operate over a single tensor at each rank. 
The simplest thing we can do is to send a tensor from one rank and receive on the other:

<figure>
<video width="80%" autoplay loop muted>
  <source src="jupyter/media/videos/jupyter/1080p60/CollectiveP2P.mp4" type="video/mp4">
</video>
<figcaption aria-hidden="true">P2P send, circles correspond to ranks and squares to tensors.</figcaption>
</figure>

Now let's suppose we want to synchronise tensors distributed over several ranks.
One way to do this is with an <mark>AllToAll</mark> collective, a complete graph of p2p sends:

<figure>
<video width="80%" autoplay loop muted>
  <source src="jupyter/media/videos/jupyter/1080p60/CollectiveAllToAll.mp4" type="video/mp4">
</video>
<figcaption aria-hidden="true">$N=4$ rank AllToAll</figcaption>
</figure>

This isn't always super efficient: $N$ ranks require $N(N-1)$ sends, some of which may share underlying network links and bottleneck one another. 
Moreover, we often only need an *aggregate* of the distributed tensors -- for example we might want to average some parameters we've replicated across the ranks. 
So how might we accomplish this with less bandwidth? 
If each rank **reduces** (applying an associative^[Floating point addition is not associative, but in practice the difference is small enough to be safely ignored.] operator e.g. sum, min, max, etc.) the tensor it receives with its own local tensor, before passing the result onto the next rank, we obtain a **ring-based** <mark>Reduce</mark> collective:

<figure>
<video width="80%" autoplay loop muted>
  <source src="jupyter/media/videos/jupyter/1080p60/CollectiveReduce.mp4" type="video/mp4">
</video>
<figcaption aria-hidden="true">$N=4$ rank Reduce</figcaption>
</figure>

After completing one loop around the ring, we've reduced all of the tensors into a single tensor -- but this result is only held in the last rank.
We need to complete another loop so that each rank holds a replica of the resulting tensor. 
This is the <mark>Broadcast</mark> collective:

<figure>
<video width="80%" autoplay loop muted>
  <source src="jupyter/media/videos/jupyter/1080p60/CollectiveBroadcast.mp4" type="video/mp4">
</video>
<figcaption aria-hidden="true">$N=4$ rank Broadcast</figcaption>
</figure>

Notice that, in the latter two collectives, only one rank/link at a time is busy, with the rest idle.
We can use pipelining to get better throughput: we split the tensor into $N$ chunks, with the $i^\text{th}$ rank at the start (or **root**) of the ring corresponding to the $i^\text{th}$ chunk.
The pipelined analogs of Reduce and Broadcast are <mark>ReduceScatter</mark> and <mark>AllGather</mark> respectively.
Sequencing the two together results in the composite <mark>AllReduce</mark> collective:

<figure>
<video width="80%" autoplay loop muted>
  <source src="jupyter/media/videos/jupyter/1080p60/CollectiveAllReduce.mp4" type="video/mp4">
</video>
<figcaption aria-hidden="true">$N=4$ rank AllReduce</figcaption>
</figure>

The ReduceScatter and AllGather collectives correspond to the first and second loops in the above animation.
Notice we obtain the same result we would have had with an AllToAll followed by local reductions at each rank.
However, with its use of a ring, AllReduce improves communication overhead by an order of magnitude to $2(N-1)$ sends.

Though Ring AllReduce is bandwidth optimal, its end-to-end latency scales linearly with the number of ranks. A lower latency, tree-based alternative is discussed in [section X]().
<!-- TODO: summary table of each collective, overhead, latency, etc -->

# 3. Parallelisation Paradigms
In order to have some notion of correctness, let's define a distributed algorithm to be **locally consistent** if it is mathematically equivalent to local training.

### Distributed Data Parallel (DDP)
[[Further Reading]](https://www.vldb.org/pvldb/vol13/p3005-li.pdf)

As its name would imply, DDP splits our *dataset* across ranks (each with a copy of the model), with periodic synchronisation to ensure model replicas are consistent. DDP is useful when our model is still small enough to fit on a single GPU, but we'd like to speed up training by having several GPUs work on a single batch in parallel.

We described local training in [Section 1](#how-do-you-train-a-pytorch-neural-network): at each iteration we load the next batch, perform a forward pass while caching each layer's activations, and calculate the loss. Then we run the backward pass to calculate gradients, before our optimizer updates parameters. 

![Local training example on a batch of 6 MNIST images ([image credit](https://siboehm.com/articles/22/data-parallel-training)).](images/local_training.png)

DDP duplicates the model across $N$ ranks, splitting batches into $N$ chunks^[In practice we use $N\times$ larger batches, so that the per-GPU batch-size remains the same.] for each rank to process:

![Data parallel training example with 2 ranks ([image credit](https://siboehm.com/articles/22/data-parallel-training)).](images/ddp_training.png)

Without any communication overhead, this should result in a linear $N\times$ speedup. 

#### DDP in PyTorch

### Fully-Sharded Data Parallel (FSDP)
#### FSDP in PyTorch

### Pipeline Parallel (PP)
#### PP in PyTorch

### Tensor Parallel (TP)
#### TP in PyTorch

### Context Prallel (CP)
#### CP in PyTorch
